{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kermitstart/marine-fish-recognition/blob/master/colab_fish_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5667efd7",
      "metadata": {
        "id": "5667efd7"
      },
      "source": [
        "# 🐟 海洋鱼类识别模型 - Colab训练版\n",
        "\n",
        "本notebook专为Google Colab设计，用于训练海洋鱼类单鱼识别模型。\n",
        "\n",
        "## 📋 训练计划\n",
        "1. **环境设置**: 配置GPU、安装依赖\n",
        "2. **数据准备**: 上传和处理增强数据集\n",
        "3. **模型训练**: 使用EfficientNet/ResNet进行迁移学习\n",
        "4. **性能评估**: 验证模型效果\n",
        "5. **模型导出**: 保存用于部署的模型\n",
        "\n",
        "## ⚡ 使用前准备\n",
        "- 确保启用GPU运行时 (Runtime → Change runtime type → GPU)\n",
        "- 准备好增强数据集的压缩包\n",
        "- 预计训练时间: 2-4小时"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "856d536b",
      "metadata": {
        "id": "856d536b"
      },
      "source": [
        "## 🔧 项目设置\n",
        "\n",
        "### 从GitHub克隆项目\n",
        "本notebook会自动从GitHub克隆海洋鱼类识别项目到Google Drive，确保数据和代码的持久保存。\n",
        "\n",
        "**使用步骤：**\n",
        "1. 运行下方代码挂载Google Drive\n",
        "2. 自动克隆项目到Drive中\n",
        "3. 设置工作目录并开始训练\n",
        "\n",
        "**注意：** 请确保您有足够的Drive空间（建议5GB以上）"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6460aae4",
      "metadata": {
        "id": "6460aae4"
      },
      "source": [
        "## 1. 环境设置和GPU检查"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "006b5bae",
      "metadata": {
        "id": "006b5bae"
      },
      "outputs": [],
      "source": [
        "# === 步骤1: 挂载Google Drive ===\n",
        "from google.colab import drive\n",
        "import os\n",
        "import subprocess\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "# 挂载Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"✅ Google Drive 挂载成功\")\n",
        "\n",
        "# 设置项目路径\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
        "PROJECT_DIR = f\"{DRIVE_ROOT}/MarineFish_recognition\"\n",
        "GITHUB_REPO = \"kermitstart/marine-fish-recognition\"  # 🔴 请替换为实际的GitHub仓库地址\n",
        "\n",
        "print(f\"项目将保存到: {PROJECT_DIR}\")\n",
        "\n",
        "# === 步骤2: 克隆或更新项目 ===\n",
        "def clone_or_update_project():\n",
        "    \"\"\"从GitHub克隆或更新项目到Google Drive\"\"\"\n",
        "    try:\n",
        "        if os.path.exists(PROJECT_DIR):\n",
        "            print(\"🔄 项目目录已存在，更新项目...\")\n",
        "            os.chdir(PROJECT_DIR)\n",
        "            # 拉取最新更改\n",
        "            result = subprocess.run([\"git\", \"pull\"], capture_output=True, text=True)\n",
        "            if result.returncode == 0:\n",
        "                print(\"✅ 项目更新成功\")\n",
        "            else:\n",
        "                print(f\"⚠️  项目更新失败: {result.stderr}\")\n",
        "        else:\n",
        "            print(\"📥 从GitHub克隆项目...\")\n",
        "            os.chdir(DRIVE_ROOT)\n",
        "            # 克隆项目\n",
        "            result = subprocess.run([\n",
        "                \"git\", \"clone\",\n",
        "                f\"https://github.com/{GITHUB_REPO}.git\",\n",
        "                \"MarineFish_recognition\"\n",
        "            ], capture_output=True, text=True)\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"✅ 项目克隆成功\")\n",
        "            else:\n",
        "                print(f\"❌ 项目克隆失败: {result.stderr}\")\n",
        "                print(\"请检查GitHub仓库地址是否正确\")\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"❌ 项目设置失败: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# 执行项目克隆/更新\n",
        "success = clone_or_update_project()\n",
        "\n",
        "if success:\n",
        "    # 切换到项目目录\n",
        "    os.chdir(PROJECT_DIR)\n",
        "    print(f\"📂 当前工作目录: {os.getcwd()}\")\n",
        "\n",
        "    # 检查项目结构\n",
        "    if os.path.exists(\"fish_backbone\"):\n",
        "        print(\"✅ 项目结构检查通过\")\n",
        "        print(\"📁 发现以下关键目录:\")\n",
        "        for item in [\"fish_backbone\", \"compact_dataset\", \"dataset\", \"fish_backbone/mini_dataset\"]:\n",
        "            if os.path.exists(item):\n",
        "                print(f\"   ✅ {item}\")\n",
        "            else:\n",
        "                print(f\"   ⚠️  {item} (未找到)\")\n",
        "    else:\n",
        "        print(\"❌ 项目结构异常，请检查GitHub仓库\")\n",
        "else:\n",
        "    print(\"❌ 无法继续，请检查网络连接和GitHub仓库设置\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5d5e624",
      "metadata": {
        "id": "f5d5e624"
      },
      "source": [
        "## 2. 从GitHub克隆项目和数据集\n",
        "\n",
        "### 方案A: 从GitHub克隆完整项目（推荐）\n",
        "1. 将整个项目（包括增强数据集）上传到GitHub\n",
        "2. 在Colab中直接克隆和使用\n",
        "\n",
        "### 方案B: 挂载Google Drive\n",
        "1. 将数据集上传到Google Drive\n",
        "2. 在Colab中挂载Drive访问数据\n",
        "\n",
        "选择下面对应的代码块执行："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14255fe0",
      "metadata": {
        "id": "14255fe0"
      },
      "outputs": [],
      "source": [
        "# 方案A: 从GitHub克隆项目 (推荐)\n",
        "def clone_from_github():\n",
        "    \"\"\"从GitHub克隆项目\"\"\"\n",
        "\n",
        "    # 替换为您的GitHub仓库URL\n",
        "    GITHUB_REPO_URL = \"https://github.com/YOUR_USERNAME/MarineFish_recognition.git\"\n",
        "    PROJECT_NAME = \"MarineFish_recognition\"\n",
        "\n",
        "    print(\"📥 从GitHub克隆项目...\")\n",
        "\n",
        "    # 克隆仓库\n",
        "    if not os.path.exists(PROJECT_NAME):\n",
        "        print(f\"🔄 正在克隆: {GITHUB_REPO_URL}\")\n",
        "        !git clone {GITHUB_REPO_URL}\n",
        "        print(f\"✅ 项目克隆完成: {PROJECT_NAME}\")\n",
        "    else:\n",
        "        print(f\"📁 项目已存在: {PROJECT_NAME}\")\n",
        "        # 拉取最新更改\n",
        "        !cd {PROJECT_NAME} && git pull\n",
        "        print(\"✅ 项目已更新到最新版本\")\n",
        "\n",
        "    # 自动检测数据集\n",
        "    return auto_detect_dataset(PROJECT_NAME)\n",
        "\n",
        "# 方案B: 挂载Google Drive\n",
        "def mount_google_drive():\n",
        "    \"\"\"挂载Google Drive并访问数据集\"\"\"\n",
        "\n",
        "    print(\"📱 挂载Google Drive...\")\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # 数据集在Drive中的路径 (请根据实际情况修改)\n",
        "    drive_dataset_path = \"/content/drive/MyDrive/MarineFish_Dataset/augmented_dataset\"\n",
        "\n",
        "    if os.path.exists(drive_dataset_path):\n",
        "        print(\"✅ Google Drive挂载成功，找到数据集\")\n",
        "\n",
        "        classes = [d for d in os.listdir(drive_dataset_path)\n",
        "                  if os.path.isdir(os.path.join(drive_dataset_path, d)) and not d.startswith('.')]\n",
        "\n",
        "        print(f\"📊 发现 {len(classes)} 个鱼类类别\")\n",
        "        return True, classes, drive_dataset_path\n",
        "    else:\n",
        "        print(\"❌ 未在Google Drive中找到数据集\")\n",
        "        print(f\"请确保数据集在: {drive_dataset_path}\")\n",
        "        return False, [], \"\"\n",
        "\n",
        "# 选择使用的方案\n",
        "USE_GITHUB = True  # 设置为True使用GitHub，False使用Google Drive\n",
        "\n",
        "print(\"🚀 开始数据集准备...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if USE_GITHUB:\n",
        "    print(\"📂 使用方案A: GitHub克隆\")\n",
        "    print(\"⚠️  请先修改上面的GITHUB_REPO_URL为您的仓库地址\")\n",
        "    success, class_names, data_path = clone_from_github()\n",
        "else:\n",
        "    print(\"📂 使用方案B: Google Drive\")\n",
        "    success, class_names, data_path = mount_google_drive()\n",
        "\n",
        "if success:\n",
        "    print(\"\\n🎯 数据集准备就绪，可以开始训练！\")\n",
        "    NUM_CLASSES = len(class_names)\n",
        "    print(f\"分类类别数: {NUM_CLASSES}\")\n",
        "\n",
        "    # 显示详细统计\n",
        "    total_images = 0\n",
        "    print(f\"\\n📊 数据集详情:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        class_path = os.path.join(data_path, class_name)\n",
        "        if os.path.exists(class_path):\n",
        "            count = len([f for f in os.listdir(class_path)\n",
        "                        if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "            total_images += count\n",
        "            if i < 10:  # 只显示前10个类别\n",
        "                print(f\"  {i+1:2d}. {class_name:25s} - {count:3d} 张\")\n",
        "\n",
        "    if len(class_names) > 10:\n",
        "        print(f\"  ... 还有 {len(class_names)-10} 个类别\")\n",
        "\n",
        "    print(f\"\\n📈 总计: {total_images} 张图片\")\n",
        "    print(f\"平均每类: {total_images // len(class_names)} 张\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n⚠️  数据集准备失败，请检查:\")\n",
        "    if USE_GITHUB:\n",
        "        print(\"1. GitHub仓库URL是否正确\")\n",
        "        print(\"2. 仓库是否包含数据集 (augmented_dataset 或 compact_dataset.zip)\")\n",
        "        print(\"3. 网络连接是否正常\")\n",
        "    else:\n",
        "        print(\"1. Google Drive是否已授权\")\n",
        "        print(\"2. 数据集路径是否正确\")\n",
        "        print(\"3. 数据集是否已上传到Drive\")\n",
        "\n",
        "# === 步骤3: 安装依赖 ===\n",
        "import sys\n",
        "\n",
        "# 确保在项目目录中\n",
        "if not os.path.exists(\"fish_backbone\"):\n",
        "    print(\"❌ 请先运行上面的代码克隆项目\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(\"📦 安装Python依赖包...\")\n",
        "\n",
        "# 检查是否有requirements.txt\n",
        "requirements_files = [\n",
        "    \"fish_backbone/requirements.txt\",\n",
        "    \"requirements.txt\"\n",
        "]\n",
        "\n",
        "requirements_file = None\n",
        "for req_file in requirements_files:\n",
        "    if os.path.exists(req_file):\n",
        "        requirements_file = req_file\n",
        "        break\n",
        "\n",
        "if requirements_file:\n",
        "    print(f\"📋 使用 {requirements_file} 安装依赖\")\n",
        "    result = subprocess.run([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_file\n",
        "    ], capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(\"✅ 依赖安装成功\")\n",
        "    else:\n",
        "        print(f\"⚠️  部分依赖安装失败: {result.stderr}\")\n",
        "        print(\"尝试手动安装核心依赖...\")\n",
        "\n",
        "        # 手动安装核心依赖\n",
        "        core_packages = [\n",
        "            \"torch\", \"torchvision\", \"torchaudio\",\n",
        "            \"pillow\", \"numpy\", \"matplotlib\",\n",
        "            \"opencv-python\", \"scikit-learn\",\n",
        "            \"tqdm\", \"flask\"\n",
        "        ]\n",
        "\n",
        "        for package in core_packages:\n",
        "            print(f\"安装 {package}...\")\n",
        "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package],\n",
        "                         capture_output=True, text=True)\n",
        "else:\n",
        "    print(\"📋 未找到requirements.txt，安装基础依赖...\")\n",
        "    # 基础依赖列表\n",
        "    packages = [\n",
        "        \"torch\", \"torchvision\", \"torchaudio\",\n",
        "        \"pillow\", \"numpy\", \"matplotlib\", \"opencv-python\",\n",
        "        \"scikit-learn\", \"tqdm\", \"flask\", \"requests\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"安装 {package}...\")\n",
        "        result = subprocess.run([\n",
        "            sys.executable, \"-m\", \"pip\", \"install\", package\n",
        "        ], capture_output=True, text=True)\n",
        "\n",
        "        if result.returncode == 0:\n",
        "            print(f\"✅ {package} 安装成功\")\n",
        "        else:\n",
        "            print(f\"⚠️  {package} 安装失败\")\n",
        "\n",
        "print(\"🎯 检查GPU可用性...\")\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"✅ GPU可用: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"   GPU内存: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    else:\n",
        "        print(\"⚠️  GPU不可用，将使用CPU训练（速度较慢）\")\n",
        "except ImportError:\n",
        "    print(\"⚠️  PyTorch未正确安装\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9daa9ce",
      "metadata": {
        "id": "f9daa9ce"
      },
      "outputs": [],
      "source": [
        "# 方案C: 处理GitHub上的紧凑数据集 (适合较小项目)\n",
        "def extract_compact_dataset(project_dir):\n",
        "    \"\"\"解压GitHub项目中的紧凑数据集\"\"\"\n",
        "\n",
        "    compact_zip = os.path.join(project_dir, \"compact_dataset.zip\")\n",
        "\n",
        "    if os.path.exists(compact_zip):\n",
        "        print(\"📦 发现紧凑数据集压缩包，开始解压...\")\n",
        "\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(compact_zip, 'r') as zip_ref:\n",
        "            zip_ref.extractall(project_dir)\n",
        "\n",
        "        dataset_path = os.path.join(project_dir, \"compact_dataset\")\n",
        "\n",
        "        if os.path.exists(dataset_path):\n",
        "            classes = [d for d in os.listdir(dataset_path)\n",
        "                      if os.path.isdir(os.path.join(dataset_path, d)) and not d.startswith('.')]\n",
        "\n",
        "            print(f\"✅ 紧凑数据集解压完成\")\n",
        "            print(f\"📊 发现 {len(classes)} 个鱼类类别\")\n",
        "\n",
        "            # 统计图片数量\n",
        "            total_images = 0\n",
        "            for class_name in classes[:5]:\n",
        "                class_path = os.path.join(dataset_path, class_name)\n",
        "                count = len([f for f in os.listdir(class_path)\n",
        "                            if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
        "                total_images += count\n",
        "                print(f\"  - {class_name}: {count} 张图片\")\n",
        "\n",
        "            if len(classes) > 5:\n",
        "                print(f\"  ... 还有 {len(classes)-5} 个类别\")\n",
        "\n",
        "            print(f\"📈 总计: 约 {total_images * len(classes) // 5} 张图片\")\n",
        "\n",
        "            return True, classes, dataset_path\n",
        "        else:\n",
        "            print(\"❌ 解压后未找到数据集目录\")\n",
        "            return False, [], \"\"\n",
        "    else:\n",
        "        print(\"❌ 未找到紧凑数据集压缩包\")\n",
        "        return False, [], \"\"\n",
        "\n",
        "# 自动检测数据集类型\n",
        "def auto_detect_dataset(project_dir):\n",
        "    \"\"\"自动检测可用的数据集\"\"\"\n",
        "\n",
        "    # 检查完整增强数据集\n",
        "    full_dataset = os.path.join(project_dir, \"augmented_dataset\")\n",
        "    if os.path.exists(full_dataset):\n",
        "        print(\"🎯 找到完整增强数据集\")\n",
        "        classes = [d for d in os.listdir(full_dataset)\n",
        "                  if os.path.isdir(os.path.join(full_dataset, d)) and not d.startswith('.')]\n",
        "        return True, classes, full_dataset\n",
        "\n",
        "    # 检查紧凑数据集压缩包\n",
        "    compact_zip = os.path.join(project_dir, \"compact_dataset.zip\")\n",
        "    if os.path.exists(compact_zip):\n",
        "        print(\"📦 找到紧凑数据集压缩包\")\n",
        "        return extract_compact_dataset(project_dir)\n",
        "\n",
        "    # 检查已解压的紧凑数据集\n",
        "    compact_dataset = os.path.join(project_dir, \"compact_dataset\")\n",
        "    if os.path.exists(compact_dataset):\n",
        "        print(\"📁 找到紧凑数据集目录\")\n",
        "        classes = [d for d in os.listdir(compact_dataset)\n",
        "                  if os.path.isdir(os.path.join(compact_dataset, d)) and not d.startswith('.')]\n",
        "        return True, classes, compact_dataset\n",
        "\n",
        "    print(\"❌ 未找到任何可用的数据集\")\n",
        "    return False, [], \"\"\n",
        "\n",
        "# === 步骤4: 选择数据集 ===\n",
        "import json\n",
        "\n",
        "print(\"🎯 检测可用数据集...\")\n",
        "\n",
        "# 检查可用的数据集\n",
        "available_datasets = {}\n",
        "\n",
        "# 检查完整数据集\n",
        "if os.path.exists(\"dataset\"):\n",
        "    dataset_size = len([f for f in os.listdir(\"dataset\") if os.path.isdir(os.path.join(\"dataset\", f))])\n",
        "    available_datasets[\"完整数据集\"] = {\n",
        "        \"path\": \"dataset\",\n",
        "        \"classes\": dataset_size,\n",
        "        \"description\": \"完整的海洋鱼类数据集，类别最多但训练时间较长\"\n",
        "    }\n",
        "\n",
        "# 检查紧凑数据集\n",
        "if os.path.exists(\"compact_dataset\"):\n",
        "    compact_size = len([f for f in os.listdir(\"compact_dataset\") if os.path.isdir(os.path.join(\"compact_dataset\", f))])\n",
        "    available_datasets[\"紧凑数据集\"] = {\n",
        "        \"path\": \"compact_dataset\",\n",
        "        \"classes\": compact_size,\n",
        "        \"description\": \"精选的紧凑数据集，平衡了数据量和训练效率\"\n",
        "    }\n",
        "\n",
        "# 检查mini数据集\n",
        "if os.path.exists(\"fish_backbone/mini_dataset\"):\n",
        "    mini_size = len([f for f in os.listdir(\"fish_backbone/mini_dataset\") if os.path.isdir(os.path.join(\"fish_backbone/mini_dataset\", f))])\n",
        "    available_datasets[\"Mini数据集\"] = {\n",
        "        \"path\": \"fish_backbone/mini_dataset\",\n",
        "        \"classes\": mini_size,\n",
        "        \"description\": \"快速测试用的小型数据集，训练速度最快\"\n",
        "    }\n",
        "\n",
        "# 显示可用数据集\n",
        "print(\"📊 可用数据集:\")\n",
        "for name, info in available_datasets.items():\n",
        "    print(f\"   🔹 {name}: {info['classes']}个类别 - {info['description']}\")\n",
        "\n",
        "# === 数据集选择配置 ===\n",
        "# 🔴 在这里选择要使用的数据集类型\n",
        "DATASET_CHOICE = \"Mini数据集\"  # 可选: \"完整数据集\", \"紧凑数据集\", \"Mini数据集\"\n",
        "\n",
        "if DATASET_CHOICE not in available_datasets:\n",
        "    print(f\"❌ 选择的数据集 '{DATASET_CHOICE}' 不可用\")\n",
        "    print(\"可用选项:\", list(available_datasets.keys()))\n",
        "    DATASET_CHOICE = list(available_datasets.keys())[0]  # 使用第一个可用的数据集\n",
        "    print(f\"🔄 自动选择: {DATASET_CHOICE}\")\n",
        "\n",
        "# 设置数据集路径\n",
        "DATASET_PATH = available_datasets[DATASET_CHOICE][\"path\"]\n",
        "NUM_CLASSES = available_datasets[DATASET_CHOICE][\"classes\"]\n",
        "\n",
        "print(f\"\\n✅ 选择数据集: {DATASET_CHOICE}\")\n",
        "print(f\"📂 数据集路径: {DATASET_PATH}\")\n",
        "print(f\"🏷️  类别数量: {NUM_CLASSES}\")\n",
        "\n",
        "# 检查对应的配置文件\n",
        "config_files = {\n",
        "    \"dataset\": \"fish_backbone/dataset_config.json\",\n",
        "    \"compact_dataset\": \"fish_backbone/dataset_config.json\",\n",
        "    \"fish_backbone/mini_dataset\": \"fish_backbone/mini_dataset_config.json\"\n",
        "}\n",
        "\n",
        "config_file = config_files.get(DATASET_PATH)\n",
        "if config_file and os.path.exists(config_file):\n",
        "    print(f\"📋 找到配置文件: {config_file}\")\n",
        "    with open(config_file, 'r', encoding='utf-8') as f:\n",
        "        dataset_config = json.load(f)\n",
        "    print(f\"   类别映射已加载: {len(dataset_config.get('class_names', []))}个类别\")\n",
        "else:\n",
        "    print(\"⚠️  未找到对应的配置文件，将动态生成\")\n",
        "\n",
        "# 检查数据集内容\n",
        "print(f\"\\n🔍 检查数据集内容...\")\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    classes = [d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))]\n",
        "    print(f\"   发现 {len(classes)} 个类别:\")\n",
        "    for i, cls in enumerate(sorted(classes)[:10]):  # 只显示前10个\n",
        "        sample_count = len(os.listdir(os.path.join(DATASET_PATH, cls)))\n",
        "        print(f\"     {i+1:2d}. {cls}: {sample_count} 张图片\")\n",
        "    if len(classes) > 10:\n",
        "        print(f\"     ... 还有 {len(classes)-10} 个类别\")\n",
        "else:\n",
        "    print(\"❌ 数据集路径不存在\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb56c02",
      "metadata": {
        "id": "deb56c02"
      },
      "source": [
        "## 3. 数据加载器定义"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fabdd53",
      "metadata": {
        "id": "2fabdd53"
      },
      "outputs": [],
      "source": [
        "class FishDataset(Dataset):\n",
        "    \"\"\"海洋鱼类数据集类\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = sorted([d for d in os.listdir(root_dir)\n",
        "                              if os.path.isdir(os.path.join(root_dir, d)) and not d.startswith('.')])\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "\n",
        "        # 构建图片路径和标签列表\n",
        "        self.samples = []\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(root_dir, class_name)\n",
        "            class_idx = self.class_to_idx[class_name]\n",
        "\n",
        "            for img_name in os.listdir(class_dir):\n",
        "                if img_name.endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    img_path = os.path.join(class_dir, img_name)\n",
        "                    self.samples.append((img_path, class_idx))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "\n",
        "        # 加载图片\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# 定义数据变换\n",
        "def get_transforms():\n",
        "    # 训练时的数据增强\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    # 验证/测试时的变换\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_transform\n",
        "\n",
        "# 创建数据集和数据加载器\n",
        "if success:\n",
        "    train_transform, val_transform = get_transforms()\n",
        "\n",
        "    # 创建完整数据集\n",
        "    full_dataset = FishDataset(data_path, transform=val_transform)\n",
        "\n",
        "    # 划分训练集、验证集、测试集 (70%, 20%, 10%)\n",
        "    dataset_size = len(full_dataset)\n",
        "    train_size = int(0.7 * dataset_size)\n",
        "    val_size = int(0.2 * dataset_size)\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = random_split(\n",
        "        full_dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    # 为训练集设置数据增强\n",
        "    train_dataset.dataset.transform = train_transform\n",
        "\n",
        "    # 创建数据加载器\n",
        "    BATCH_SIZE = 32\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"📊 数据集划分完成:\")\n",
        "    print(f\"  训练集: {len(train_dataset)} 张 ({len(train_dataset)/dataset_size*100:.1f}%)\")\n",
        "    print(f\"  验证集: {len(val_dataset)} 张 ({len(val_dataset)/dataset_size*100:.1f}%)\")\n",
        "    print(f\"  测试集: {len(test_dataset)} 张 ({len(test_dataset)/dataset_size*100:.1f}%)\")\n",
        "    print(f\"  批次大小: {BATCH_SIZE}\")\n",
        "\n",
        "    # 保存类别信息\n",
        "    class_info = {\n",
        "        'classes': class_names,\n",
        "        'class_to_idx': full_dataset.class_to_idx,\n",
        "        'num_classes': len(class_names)\n",
        "    }\n",
        "\n",
        "    with open('class_info.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(class_info, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(\"✅ 数据加载器创建完成\")\n",
        "else:\n",
        "    print(\"⚠️  请先准备数据集\")\n",
        "\n",
        "# === 步骤5: 模型定义 ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# 添加项目路径到Python路径\n",
        "sys.path.append('fish_backbone')\n",
        "\n",
        "print(f\"🧠 定义模型架构 (类别数: {NUM_CLASSES})\")\n",
        "\n",
        "class FishClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(FishClassifier, self).__init__()\n",
        "        # 使用预训练的ResNet18作为骨干网络\n",
        "        self.backbone = models.resnet18(pretrained=True)\n",
        "\n",
        "        # 替换最后的分类层\n",
        "        in_features = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "# 创建模型实例\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = FishClassifier(num_classes=NUM_CLASSES).to(device)\n",
        "\n",
        "print(f\"✅ 模型创建成功\")\n",
        "print(f\"📱 设备: {device}\")\n",
        "print(f\"🔢 参数总数: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# 定义数据变换\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(\"🔄 数据变换定义完成\")\n",
        "\n",
        "# 检查是否有预训练模型可以加载\n",
        "model_files = [\n",
        "    \"fish_backbone/marine_fish_model.pth\",\n",
        "    \"fish_backbone/best_model.pth\",\n",
        "    \"marine_fish_model.pth\"\n",
        "]\n",
        "\n",
        "pretrained_model = None\n",
        "for model_file in model_files:\n",
        "    if os.path.exists(model_file):\n",
        "        pretrained_model = model_file\n",
        "        break\n",
        "\n",
        "if pretrained_model:\n",
        "    try:\n",
        "        print(f\"🔄 尝试加载预训练模型: {pretrained_model}\")\n",
        "        checkpoint = torch.load(pretrained_model, map_location=device)\n",
        "\n",
        "        # 检查模型结构兼容性\n",
        "        if hasattr(checkpoint, 'keys') and 'state_dict' in checkpoint:\n",
        "            model_state = checkpoint['state_dict']\n",
        "        else:\n",
        "            model_state = checkpoint\n",
        "\n",
        "        # 检查分类层的维度\n",
        "        fc_weight_shape = model_state.get('backbone.fc.weight', model_state.get('fc.weight'))\n",
        "        if fc_weight_shape is not None and fc_weight_shape.shape[0] == NUM_CLASSES:\n",
        "            model.load_state_dict(model_state)\n",
        "            print(\"✅ 预训练模型加载成功\")\n",
        "        else:\n",
        "            print(f\"⚠️  预训练模型类别数不匹配 (需要: {NUM_CLASSES})，使用随机初始化\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  预训练模型加载失败: {str(e)}\")\n",
        "        print(\"使用随机初始化的模型\")\n",
        "else:\n",
        "    print(\"📝 未找到预训练模型，使用随机初始化\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef6ef1f",
      "metadata": {
        "id": "0ef6ef1f"
      },
      "source": [
        "## 4. 模型定义\n",
        "\n",
        "选择预训练模型进行迁移学习。推荐使用EfficientNet-B0以获得最佳的速度和精度平衡。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed6011e8",
      "metadata": {
        "id": "ed6011e8"
      },
      "outputs": [],
      "source": [
        "# 安装efficientnet（如果需要）\n",
        "try:\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    print(\"✅ EfficientNet 已安装\")\n",
        "except ImportError:\n",
        "    print(\"📦 正在安装 EfficientNet...\")\n",
        "    !pip install efficientnet_pytorch\n",
        "    from efficientnet_pytorch import EfficientNet\n",
        "    print(\"✅ EfficientNet 安装完成\")\n",
        "\n",
        "def create_model(model_name='efficientnet-b0', num_classes=NUM_CLASSES, pretrained=True):\n",
        "    \"\"\"\n",
        "    创建预训练模型\n",
        "\n",
        "    Args:\n",
        "        model_name: 模型名称 ('efficientnet-b0', 'resnet50', 'resnet101')\n",
        "        num_classes: 分类类别数\n",
        "        pretrained: 是否使用预训练权重\n",
        "    \"\"\"\n",
        "\n",
        "    if model_name.startswith('efficientnet'):\n",
        "        if pretrained:\n",
        "            model = EfficientNet.from_pretrained(model_name, num_classes=num_classes)\n",
        "        else:\n",
        "            model = EfficientNet.from_name(model_name, num_classes=num_classes)\n",
        "\n",
        "        print(f\"🤖 创建 {model_name} 模型\")\n",
        "\n",
        "    elif model_name == 'resnet50':\n",
        "        model = models.resnet50(pretrained=pretrained)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        print(f\"🤖 创建 ResNet50 模型\")\n",
        "\n",
        "    elif model_name == 'resnet101':\n",
        "        model = models.resnet101(pretrained=pretrained)\n",
        "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "        print(f\"🤖 创建 ResNet101 模型\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"不支持的模型: {model_name}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# 模型选择\n",
        "MODEL_NAME = 'efficientnet-b0'  # 可以改为 'resnet50', 'resnet101'\n",
        "\n",
        "if success:\n",
        "    # 创建模型\n",
        "    model = create_model(MODEL_NAME, NUM_CLASSES)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # 计算模型参数\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"📊 模型信息:\")\n",
        "    print(f\"  模型: {MODEL_NAME}\")\n",
        "    print(f\"  类别数: {NUM_CLASSES}\")\n",
        "    print(f\"  总参数: {total_params:,}\")\n",
        "    print(f\"  可训练参数: {trainable_params:,}\")\n",
        "    print(f\"  设备: {device}\")\n",
        "\n",
        "    # 定义损失函数和优化器\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "    # 学习率调度器\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50, eta_min=1e-6)\n",
        "\n",
        "    print(\"✅ 模型和训练组件创建完成\")\n",
        "else:\n",
        "    print(\"⚠️  请先准备数据集\")\n",
        "\n",
        "# === 步骤6: 数据加载 ===\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"📊 准备数据加载...\")\n",
        "\n",
        "class FishDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        try:\n",
        "            image_path = self.image_paths[idx]\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            label = self.labels[idx]\n",
        "\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "            return image, label\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  加载图片失败: {image_path}, 错误: {str(e)}\")\n",
        "            # 返回一个空白图片作为备用\n",
        "            blank_image = Image.new('RGB', (224, 224), color='white')\n",
        "            if self.transform:\n",
        "                blank_image = self.transform(blank_image)\n",
        "            return blank_image, self.labels[idx]\n",
        "\n",
        "# 扫描数据集\n",
        "print(f\"🔍 扫描数据集: {DATASET_PATH}\")\n",
        "image_paths = []\n",
        "labels = []\n",
        "class_names = []\n",
        "\n",
        "if os.path.exists(DATASET_PATH):\n",
        "    class_dirs = sorted([d for d in os.listdir(DATASET_PATH)\n",
        "                        if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
        "\n",
        "    print(f\"发现 {len(class_dirs)} 个类别\")\n",
        "\n",
        "    for class_idx, class_name in enumerate(class_dirs):\n",
        "        class_path = os.path.join(DATASET_PATH, class_name)\n",
        "        class_names.append(class_name)\n",
        "\n",
        "        # 获取该类别的所有图片\n",
        "        image_files = [f for f in os.listdir(class_path)\n",
        "                      if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "        for image_file in image_files:\n",
        "            image_path = os.path.join(class_path, image_file)\n",
        "            image_paths.append(image_path)\n",
        "            labels.append(class_idx)\n",
        "\n",
        "        print(f\"  {class_name}: {len(image_files)} 张图片\")\n",
        "\n",
        "    print(f\"\\n📈 数据集统计:\")\n",
        "    print(f\"   总图片数: {len(image_paths)}\")\n",
        "    print(f\"   类别数: {len(class_names)}\")\n",
        "    print(f\"   平均每类: {len(image_paths)/len(class_names):.1f} 张\")\n",
        "\n",
        "    # 数据集划分\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "        image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    print(f\"\\n🔄 数据集划分:\")\n",
        "    print(f\"   训练集: {len(train_paths)} 张\")\n",
        "    print(f\"   验证集: {len(val_paths)} 张\")\n",
        "\n",
        "    # 创建数据集\n",
        "    train_dataset = FishDataset(train_paths, train_labels, train_transform)\n",
        "    val_dataset = FishDataset(val_paths, val_labels, val_transform)\n",
        "\n",
        "    # 创建数据加载器\n",
        "    batch_size = 32 if len(image_paths) > 1000 else 16  # 根据数据集大小调整批次大小\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"✅ 数据加载器创建成功\")\n",
        "    print(f\"   批次大小: {batch_size}\")\n",
        "    print(f\"   训练批次数: {len(train_loader)}\")\n",
        "    print(f\"   验证批次数: {len(val_loader)}\")\n",
        "\n",
        "    # 保存类别名称映射\n",
        "    class_mapping = {i: name for i, name in enumerate(class_names)}\n",
        "    print(f\"\\n🏷️  类别映射:\")\n",
        "    for i, name in list(class_mapping.items())[:5]:\n",
        "        print(f\"   {i}: {name}\")\n",
        "    if len(class_mapping) > 5:\n",
        "        print(f\"   ... 还有 {len(class_mapping)-5} 个类别\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ 数据集路径不存在\")\n",
        "    raise FileNotFoundError(f\"数据集路径不存在: {DATASET_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8dd4c0",
      "metadata": {
        "id": "4a8dd4c0"
      },
      "source": [
        "## 5. 模型训练\n",
        "\n",
        "开始训练海洋鱼类识别模型。训练过程包括多个epoch，每个epoch都会在训练集上训练并在验证集上评估。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890e2d6b",
      "metadata": {
        "id": "890e2d6b"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
        "                num_epochs=20, patience=7, save_path='best_fish_model.pth'):\n",
        "    \"\"\"\n",
        "    训练模型\n",
        "\n",
        "    Args:\n",
        "        model: 待训练的模型\n",
        "        train_loader: 训练数据加载器\n",
        "        val_loader: 验证数据加载器\n",
        "        criterion: 损失函数\n",
        "        optimizer: 优化器\n",
        "        scheduler: 学习率调度器\n",
        "        num_epochs: 训练轮数\n",
        "        patience: 早停耐心值\n",
        "        save_path: 模型保存路径\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"🚀 开始训练，共 {num_epochs} 个epoch\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # 记录训练历史\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': [],\n",
        "        'lr': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time = time.time()\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} | LR: {current_lr:.2e}\")\n",
        "\n",
        "        # 训练阶段\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        train_pbar = tqdm(train_loader, desc=\"训练\", leave=False)\n",
        "        for batch_idx, (data, target) in enumerate(train_pbar):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            train_total += target.size(0)\n",
        "            train_correct += (predicted == target).sum().item()\n",
        "\n",
        "            # 更新进度条\n",
        "            train_acc = 100. * train_correct / train_total\n",
        "            train_pbar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Acc': f'{train_acc:.2f}%'\n",
        "            })\n",
        "\n",
        "        # 验证阶段\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_pbar = tqdm(val_loader, desc=\"验证\", leave=False)\n",
        "            for data, target in val_pbar:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                output = model(data)\n",
        "                loss = criterion(output, target)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                val_total += target.size(0)\n",
        "                val_correct += (predicted == target).sum().item()\n",
        "\n",
        "                # 更新进度条\n",
        "                val_acc = 100. * val_correct / val_total\n",
        "                val_pbar.set_postfix({\n",
        "                    'Loss': f'{loss.item():.4f}',\n",
        "                    'Acc': f'{val_acc:.2f}%'\n",
        "                })\n",
        "\n",
        "        # 计算平均指标\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        train_accuracy = 100. * train_correct / train_total\n",
        "        val_accuracy = 100. * val_correct / val_total\n",
        "\n",
        "        # 更新学习率\n",
        "        scheduler.step()\n",
        "\n",
        "        # 记录历史\n",
        "        history['train_loss'].append(avg_train_loss)\n",
        "        history['train_acc'].append(train_accuracy)\n",
        "        history['val_loss'].append(avg_val_loss)\n",
        "        history['val_acc'].append(val_accuracy)\n",
        "        history['lr'].append(current_lr)\n",
        "\n",
        "        # 计算epoch耗时\n",
        "        epoch_time = time.time() - start_time\n",
        "\n",
        "        # 打印结果\n",
        "        print(f\"训练损失: {avg_train_loss:.4f} | 训练准确率: {train_accuracy:.2f}%\")\n",
        "        print(f\"验证损失: {avg_val_loss:.4f} | 验证准确率: {val_accuracy:.2f}%\")\n",
        "        print(f\"耗时: {epoch_time:.1f}s\")\n",
        "\n",
        "        # 保存最佳模型\n",
        "        if val_accuracy > best_val_acc:\n",
        "            best_val_acc = val_accuracy\n",
        "            patience_counter = 0\n",
        "\n",
        "            # 保存模型\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'best_val_acc': best_val_acc,\n",
        "                'class_info': class_info,\n",
        "                'model_name': MODEL_NAME\n",
        "            }, save_path)\n",
        "\n",
        "            print(f\"🎯 新的最佳验证准确率: {val_accuracy:.2f}% (已保存模型)\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # 早停检查\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"🛑 验证准确率连续 {patience} 个epoch未提升，触发早停\")\n",
        "            break\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "    print(f\"🎉 训练完成！最佳验证准确率: {best_val_acc:.2f}%\")\n",
        "\n",
        "    return history\n",
        "\n",
        "# === 步骤7: 训练配置 ===\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import time\n",
        "\n",
        "print(\"⚙️ 配置训练参数...\")\n",
        "\n",
        "# 根据数据集大小自动调整训练参数\n",
        "total_samples = len(image_paths)\n",
        "if total_samples < 500:\n",
        "    # 小数据集配置\n",
        "    NUM_EPOCHS = 20\n",
        "    LEARNING_RATE = 0.001\n",
        "    STEP_SIZE = 7\n",
        "    GAMMA = 0.1\n",
        "    print(\"📝 小数据集配置\")\n",
        "elif total_samples < 2000:\n",
        "    # 中等数据集配置\n",
        "    NUM_EPOCHS = 30\n",
        "    LEARNING_RATE = 0.001\n",
        "    STEP_SIZE = 10\n",
        "    GAMMA = 0.1\n",
        "    print(\"📝 中等数据集配置\")\n",
        "else:\n",
        "    # 大数据集配置\n",
        "    NUM_EPOCHS = 50\n",
        "    LEARNING_RATE = 0.0001\n",
        "    STEP_SIZE = 15\n",
        "    GAMMA = 0.1\n",
        "    print(\"📝 大数据集配置\")\n",
        "\n",
        "print(f\"🎯 训练配置:\")\n",
        "print(f\"   训练轮数: {NUM_EPOCHS}\")\n",
        "print(f\"   学习率: {LEARNING_RATE}\")\n",
        "print(f\"   学习率衰减步长: {STEP_SIZE}\")\n",
        "print(f\"   衰减因子: {GAMMA}\")\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "\n",
        "print(\"✅ 优化器和调度器创建成功\")\n",
        "\n",
        "# 训练记录\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    \"\"\"计算准确率\"\"\"\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total = labels.size(0)\n",
        "    correct = (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"训练一个epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "        # 显示进度\n",
        "        if batch_idx % max(1, len(train_loader) // 5) == 0:\n",
        "            print(f\"    批次 {batch_idx}/{len(train_loader)}, \"\n",
        "                  f\"损失: {loss.item():.4f}, \"\n",
        "                  f\"准确率: {calculate_accuracy(outputs, labels):.4f}\")\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_accuracy = running_accuracy / len(train_loader)\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"验证一个epoch\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    running_accuracy = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_accuracy += calculate_accuracy(outputs, labels)\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_accuracy = running_accuracy / len(val_loader)\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "print(\"🚀 准备开始训练...\")\n",
        "\n",
        "# 开始训练\n",
        "if success:\n",
        "    # 训练配置\n",
        "    PATIENCE = 10\n",
        "    MODEL_SAVE_PATH = f'best_{MODEL_NAME}_fish_model.pth'\n",
        "\n",
        "    print(f\"⚙️  训练配置:\")\n",
        "    print(f\"  最大轮数: {NUM_EPOCHS}\")\n",
        "    print(f\"  早停耐心: {PATIENCE}\")\n",
        "    print(f\"  模型保存: {MODEL_SAVE_PATH}\")\n",
        "    print()\n",
        "\n",
        "    # 开始训练\n",
        "    history = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        criterion=criterion,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        patience=PATIENCE,\n",
        "        save_path=MODEL_SAVE_PATH\n",
        "    )\n",
        "\n",
        "    # 保存训练历史\n",
        "    with open('training_history.json', 'w') as f:\n",
        "        json.dump(history, f, indent=2)\n",
        "\n",
        "    print(\"✅ 训练历史已保存\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️  请先准备数据集\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00931fdf",
      "metadata": {
        "id": "00931fdf"
      },
      "source": [
        "## 6. 训练结果可视化"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8325ef0c",
      "metadata": {
        "id": "8325ef0c"
      },
      "outputs": [],
      "source": [
        "# 绘制训练曲线\n",
        "def plot_training_history(history):\n",
        "    \"\"\"绘制训练历史曲线\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # 损失曲线\n",
        "    axes[0, 0].plot(history['train_loss'], label='训练损失', color='blue')\n",
        "    axes[0, 0].plot(history['val_loss'], label='验证损失', color='red')\n",
        "    axes[0, 0].set_title('损失曲线')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "\n",
        "    # 准确率曲线\n",
        "    axes[0, 1].plot(history['train_acc'], label='训练准确率', color='blue')\n",
        "    axes[0, 1].plot(history['val_acc'], label='验证准确率', color='red')\n",
        "    axes[0, 1].set_title('准确率曲线')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy (%)')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "\n",
        "    # 学习率曲线\n",
        "    axes[1, 0].plot(history['lr'], label='学习率', color='green')\n",
        "    axes[1, 0].set_title('学习率变化')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Learning Rate')\n",
        "    axes[1, 0].set_yscale('log')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "\n",
        "    # 最终统计\n",
        "    best_val_acc = max(history['val_acc'])\n",
        "    best_epoch = history['val_acc'].index(best_val_acc)\n",
        "    final_train_acc = history['train_acc'][-1]\n",
        "    final_val_acc = history['val_acc'][-1]\n",
        "\n",
        "    stats_text = f\"\"\"训练统计:\n",
        "    最佳验证准确率: {best_val_acc:.2f}% (Epoch {best_epoch+1})\n",
        "    最终训练准确率: {final_train_acc:.2f}%\n",
        "    最终验证准确率: {final_val_acc:.2f}%\n",
        "    总训练轮数: {len(history['train_loss'])}\n",
        "    \"\"\"\n",
        "\n",
        "    axes[1, 1].text(0.1, 0.5, stats_text, fontsize=12,\n",
        "                    verticalalignment='center', transform=axes[1, 1].transAxes)\n",
        "    axes[1, 1].set_title('训练统计')\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# === 步骤8: 开始训练 ===\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "print(f\"🎯 开始训练模型...\")\n",
        "print(f\"⏱️  开始时间: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# 最佳模型保存\n",
        "best_val_accuracy = 0.0\n",
        "best_model_path = f\"{PROJECT_DIR}/best_marine_fish_model.pth\"\n",
        "\n",
        "# 训练开始时间\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start_time = time.time()\n",
        "    print(f\"\\n📍 Epoch {epoch+1}/{EPOCHS}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 训练阶段\n",
        "    print(\"🔄 训练阶段...\")\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    # 验证阶段\n",
        "    print(\"🔍 验证阶段...\")\n",
        "    val_loss, val_acc = validate_epoch(model, val_loader, criterion, device)\n",
        "\n",
        "    # 更新学习率\n",
        "    scheduler.step()\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # 记录训练结果\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # 计算epoch时间\n",
        "    epoch_time = time.time() - epoch_start_time\n",
        "\n",
        "    # 显示结果\n",
        "    print(f\"\\n📊 Epoch {epoch+1} 结果:\")\n",
        "    print(f\"   训练损失: {train_loss:.4f}, 训练准确率: {train_acc:.4f}\")\n",
        "    print(f\"   验证损失: {val_loss:.4f}, 验证准确率: {val_acc:.4f}\")\n",
        "    print(f\"   学习率: {current_lr:.6f}\")\n",
        "    print(f\"   用时: {epoch_time:.1f}秒\")\n",
        "\n",
        "    # 保存最佳模型\n",
        "    if val_acc > best_val_accuracy:\n",
        "        best_val_accuracy = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch + 1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'best_accuracy': best_val_accuracy,\n",
        "            'class_names': class_names,\n",
        "            'num_classes': NUM_CLASSES\n",
        "        }, best_model_path)\n",
        "        print(f\"✅ 保存最佳模型 (准确率: {val_acc:.4f})\")\n",
        "\n",
        "    # 预估剩余时间\n",
        "    if epoch < EPOCHS - 1:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        avg_epoch_time = elapsed_time / (epoch + 1)\n",
        "        remaining_time = avg_epoch_time * (EPOCHS - epoch - 1)\n",
        "        print(f\"⏳ 预计剩余时间: {remaining_time/60:.1f}分钟\")\n",
        "\n",
        "# 训练完成\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n🎉 训练完成!\")\n",
        "print(f\"⏱️  总用时: {total_time/60:.1f}分钟\")\n",
        "print(f\"🏆 最佳验证准确率: {best_val_accuracy:.4f}\")\n",
        "print(f\"💾 最佳模型已保存到: {best_model_path}\")\n",
        "\n",
        "# 保存最终模型\n",
        "final_model_path = f\"{PROJECT_DIR}/final_marine_fish_model.pth\"\n",
        "torch.save({\n",
        "    'epoch': EPOCHS,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'final_accuracy': val_accuracies[-1],\n",
        "    'class_names': class_names,\n",
        "    'num_classes': NUM_CLASSES,\n",
        "    'train_losses': train_losses,\n",
        "    'train_accuracies': train_accuracies,\n",
        "    'val_losses': val_losses,\n",
        "    'val_accuracies': val_accuracies\n",
        "}, final_model_path)\n",
        "print(f\"💾 最终模型已保存到: {final_model_path}\")\n",
        "\n",
        "# 绘制训练曲线\n",
        "if 'history' in locals():\n",
        "    plot_training_history(history)\n",
        "else:\n",
        "    print(\"请先完成模型训练\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e71f9674",
      "metadata": {
        "id": "e71f9674"
      },
      "source": [
        "## 7. 模型评估\n",
        "\n",
        "在测试集上评估最佳模型的性能，生成详细的分类报告和混淆矩阵。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e56b475",
      "metadata": {
        "id": "3e56b475"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, class_names, device):\n",
        "    \"\"\"在测试集上评估模型\"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    print(\"🔍 正在评估模型...\")\n",
        "    with torch.no_grad():\n",
        "        for data, target in tqdm(test_loader, desc=\"评估\"):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "    # 计算准确率\n",
        "    accuracy = accuracy_score(all_targets, all_predictions)\n",
        "\n",
        "    # 生成分类报告\n",
        "    report = classification_report(all_targets, all_predictions,\n",
        "                                 target_names=class_names,\n",
        "                                 output_dict=True)\n",
        "\n",
        "    # 生成混淆矩阵\n",
        "    cm = confusion_matrix(all_targets, all_predictions)\n",
        "\n",
        "    return accuracy, report, cm, all_predictions, all_targets\n",
        "\n",
        "def plot_confusion_matrix(cm, class_names, title='混淆矩阵'):\n",
        "    \"\"\"绘制混淆矩阵\"\"\"\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # 如果类别太多，只显示前15个\n",
        "    if len(class_names) > 15:\n",
        "        display_classes = class_names[:15]\n",
        "        display_cm = cm[:15, :15]\n",
        "        title += \" (前15个类别)\"\n",
        "    else:\n",
        "        display_classes = class_names\n",
        "        display_cm = cm\n",
        "\n",
        "    sns.heatmap(display_cm,\n",
        "                annot=True,\n",
        "                fmt='d',\n",
        "                cmap='Blues',\n",
        "                xticklabels=display_classes,\n",
        "                yticklabels=display_classes)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel('预测类别')\n",
        "    plt.ylabel('真实类别')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "def print_classification_report(report, class_names):\n",
        "    \"\"\"打印详细的分类报告\"\"\"\n",
        "\n",
        "    print(\"📊 详细分类报告:\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # 整体指标\n",
        "    print(f\"总体准确率: {report['accuracy']:.4f}\")\n",
        "    print(f\"宏平均精确率: {report['macro avg']['precision']:.4f}\")\n",
        "    print(f\"宏平均召回率: {report['macro avg']['recall']:.4f}\")\n",
        "    print(f\"宏平均F1分数: {report['macro avg']['f1-score']:.4f}\")\n",
        "    print()\n",
        "\n",
        "    # 各类别详细指标\n",
        "    print(\"各类别详细指标:\")\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"{'类别':<25} {'精确率':<10} {'召回率':<10} {'F1分数':<10} {'样本数':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for class_name in class_names:\n",
        "        if class_name in report:\n",
        "            metrics = report[class_name]\n",
        "            print(f\"{class_name:<25} {metrics['precision']:<10.4f} \"\n",
        "                  f\"{metrics['recall']:<10.4f} {metrics['f1-score']:<10.4f} \"\n",
        "                  f\"{int(metrics['support']):<10}\")\n",
        "\n",
        "# 加载最佳模型并评估\n",
        "if success and 'MODEL_SAVE_PATH' in locals():\n",
        "    if os.path.exists(MODEL_SAVE_PATH):\n",
        "        print(f\"📁 加载最佳模型: {MODEL_SAVE_PATH}\")\n",
        "\n",
        "        # 加载模型\n",
        "        checkpoint = torch.load(MODEL_SAVE_PATH, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        print(f\"✅ 模型加载完成 (最佳验证准确率: {checkpoint['best_val_acc']:.2f}%)\")\n",
        "\n",
        "        # 在测试集上评估\n",
        "        test_accuracy, test_report, test_cm, predictions, targets = evaluate_model(\n",
        "            model, test_loader, class_names, device\n",
        "        )\n",
        "\n",
        "        print(f\"\\n🎯 测试集准确率: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "        # 打印分类报告\n",
        "        print_classification_report(test_report, class_names)\n",
        "\n",
        "        # 绘制混淆矩阵\n",
        "        plot_confusion_matrix(test_cm, class_names)\n",
        "\n",
        "        # 保存评估结果\n",
        "        evaluation_results = {\n",
        "            'test_accuracy': test_accuracy,\n",
        "            'classification_report': test_report,\n",
        "            'model_path': MODEL_SAVE_PATH,\n",
        "            'model_name': MODEL_NAME,\n",
        "            'num_classes': NUM_CLASSES\n",
        "        }\n",
        "\n",
        "        with open('evaluation_results.json', 'w', encoding='utf-8') as f:\n",
        "            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"\\n✅ 评估结果已保存到 evaluation_results.json\")\n",
        "\n",
        "    else:\n",
        "        print(\"❌ 未找到训练好的模型文件\")\n",
        "else:\n",
        "    print(\"请先完成模型训练\")\n",
        "\n",
        "# === 步骤9: 结果可视化 ===\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "print(\"📈 生成训练曲线...\")\n",
        "\n",
        "# 创建训练曲线图\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# 损失曲线\n",
        "epochs_range = range(1, len(train_losses) + 1)\n",
        "ax1.plot(epochs_range, train_losses, 'b-', label='训练损失', linewidth=2)\n",
        "ax1.plot(epochs_range, val_losses, 'r-', label='验证损失', linewidth=2)\n",
        "ax1.set_title('训练和验证损失', fontsize=14, fontweight='bold')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 准确率曲线\n",
        "ax2.plot(epochs_range, train_accuracies, 'b-', label='训练准确率', linewidth=2)\n",
        "ax2.plot(epochs_range, val_accuracies, 'r-', label='验证准确率', linewidth=2)\n",
        "ax2.set_title('训练和验证准确率', fontsize=14, fontweight='bold')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{PROJECT_DIR}/training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"📊 训练统计:\")\n",
        "print(f\"   最终训练准确率: {train_accuracies[-1]:.4f}\")\n",
        "print(f\"   最终验证准确率: {val_accuracies[-1]:.4f}\")\n",
        "print(f\"   最佳验证准确率: {max(val_accuracies):.4f}\")\n",
        "print(f\"   训练曲线已保存到: {PROJECT_DIR}/training_curves.png\")\n",
        "\n",
        "# 测试模型预测\n",
        "print(\"\\n🧪 测试模型预测...\")\n",
        "\n",
        "def predict_sample_images(model, dataset, class_names, num_samples=6):\n",
        "    \"\"\"预测样本图片\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 随机选择样本\n",
        "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, idx in enumerate(indices):\n",
        "            image, true_label = dataset[idx]\n",
        "\n",
        "            # 预测\n",
        "            image_batch = image.unsqueeze(0).to(device)\n",
        "            outputs = model(image_batch)\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            predicted_label = torch.argmax(outputs, dim=1).item()\n",
        "            confidence = probabilities[0][predicted_label].item()\n",
        "\n",
        "            # 显示图片\n",
        "            # 反normalize图片用于显示\n",
        "            mean = np.array([0.485, 0.456, 0.406])\n",
        "            std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "            img_display = image.clone()\n",
        "            for c in range(3):\n",
        "                img_display[c] = img_display[c] * std[c] + mean[c]\n",
        "            img_display = torch.clamp(img_display, 0, 1)\n",
        "            img_display = img_display.permute(1, 2, 0).numpy()\n",
        "\n",
        "            axes[i].imshow(img_display)\n",
        "            axes[i].axis('off')\n",
        "\n",
        "            # 标题显示真实和预测标签\n",
        "            true_name = class_names[true_label]\n",
        "            pred_name = class_names[predicted_label]\n",
        "            color = 'green' if predicted_label == true_label else 'red'\n",
        "\n",
        "            axes[i].set_title(\n",
        "                f'真实: {true_name[:15]}...\\n预测: {pred_name[:15]}...\\n置信度: {confidence:.3f}',\n",
        "                fontsize=10, color=color, fontweight='bold'\n",
        "            )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{PROJECT_DIR}/sample_predictions.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# 执行预测测试\n",
        "predict_sample_images(model, val_dataset, class_names)\n",
        "print(f\"   样本预测结果已保存到: {PROJECT_DIR}/sample_predictions.png\")\n",
        "\n",
        "# 计算每个类别的准确率\n",
        "print(\"\\n📊 各类别性能分析...\")\n",
        "class_correct = [0] * NUM_CLASSES\n",
        "class_total = [0] * NUM_CLASSES\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        for i in range(labels.size(0)):\n",
        "            label = labels[i].item()\n",
        "            class_total[label] += 1\n",
        "            if predicted[i] == label:\n",
        "                class_correct[label] += 1\n",
        "\n",
        "print(\"🎯 各类别准确率:\")\n",
        "for i in range(min(10, NUM_CLASSES)):  # 只显示前10个类别\n",
        "    if class_total[i] > 0:\n",
        "        accuracy = class_correct[i] / class_total[i]\n",
        "        print(f\"   {class_names[i][:20]:20s}: {accuracy:.3f} ({class_correct[i]}/{class_total[i]})\")\n",
        "\n",
        "if NUM_CLASSES > 10:\n",
        "    print(f\"   ... 还有 {NUM_CLASSES-10} 个类别\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dac3b830",
      "metadata": {
        "id": "dac3b830"
      },
      "source": [
        "## 8. 模型导出\n",
        "\n",
        "将训练好的模型转换为适合部署的格式，并创建用于推理的脚本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bd6299",
      "metadata": {
        "id": "70bd6299"
      },
      "outputs": [],
      "source": [
        "# 导出模型用于部署\n",
        "def export_model_for_deployment(model, model_name, class_info, save_dir='deployment_models'):\n",
        "    \"\"\"导出用于部署的模型\"\"\"\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # 1. 保存完整模型（用于Python推理）\n",
        "    model_path = os.path.join(save_dir, f'{model_name}_complete.pth')\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'class_info': class_info,\n",
        "        'model_name': model_name,\n",
        "        'input_size': (224, 224)\n",
        "    }, model_path)\n",
        "\n",
        "    # 2. 创建推理类\n",
        "    inference_code = f'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "class FishClassifier:\n",
        "    def __init__(self, model_path, device='cpu'):\n",
        "        self.device = device\n",
        "        self.model_path = model_path\n",
        "\n",
        "        # 加载模型信息\n",
        "        checkpoint = torch.load(model_path, map_location=device)\n",
        "        self.class_info = checkpoint['class_info']\n",
        "        self.classes = self.class_info['classes']\n",
        "        self.model_name = checkpoint['model_name']\n",
        "\n",
        "        # 创建模型\n",
        "        if self.model_name.startswith('efficientnet'):\n",
        "            from efficientnet_pytorch import EfficientNet\n",
        "            self.model = EfficientNet.from_name(self.model_name, num_classes=len(self.classes))\n",
        "        elif self.model_name == 'resnet50':\n",
        "            from torchvision import models\n",
        "            self.model = models.resnet50(pretrained=False)\n",
        "            self.model.fc = nn.Linear(self.model.fc.in_features, len(self.classes))\n",
        "\n",
        "        # 加载权重\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # 定义预处理\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def predict(self, image_path_or_pil, top_k=3):\n",
        "        \"\"\"\n",
        "        预测图片类别\n",
        "\n",
        "        Args:\n",
        "            image_path_or_pil: 图片路径或PIL Image对象\n",
        "            top_k: 返回前k个预测结果\n",
        "\n",
        "        Returns:\n",
        "            list: [(class_name, confidence), ...]\n",
        "        \"\"\"\n",
        "\n",
        "        # 加载图片\n",
        "        if isinstance(image_path_or_pil, str):\n",
        "            image = Image.open(image_path_or_pil).convert('RGB')\n",
        "        else:\n",
        "            image = image_path_or_pil.convert('RGB')\n",
        "\n",
        "        # 预处理\n",
        "        input_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        # 推理\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(input_tensor)\n",
        "            probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
        "\n",
        "        # 获取top-k结果\n",
        "        top_probs, top_indices = torch.topk(probabilities, min(top_k, len(self.classes)))\n",
        "\n",
        "        results = []\n",
        "        for i in range(len(top_probs)):\n",
        "            class_idx = top_indices[i].item()\n",
        "            class_name = self.classes[class_idx]\n",
        "            confidence = top_probs[i].item()\n",
        "            results.append((class_name, confidence))\n",
        "\n",
        "        return results\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    # 初始化分类器\n",
        "    classifier = FishClassifier('{model_name}_complete.pth')\n",
        "\n",
        "    # 预测单张图片\n",
        "    # results = classifier.predict('path/to/fish/image.jpg')\n",
        "    # print(\"预测结果:\")\n",
        "    # for class_name, confidence in results:\n",
        "    #     print(f\"  {{class_name}}: {{confidence:.4f}} ({{confidence*100:.2f}}%)\")\n",
        "'''\n",
        "\n",
        "    # 保存推理代码\n",
        "    inference_path = os.path.join(save_dir, f'{model_name}_inference.py')\n",
        "    with open(inference_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(inference_code)\n",
        "\n",
        "    # 3. 创建部署说明文档\n",
        "    readme_content = f'''# 海洋鱼类识别模型部署说明\n",
        "\n",
        "## 模型信息\n",
        "- 模型名称: {model_name}\n",
        "- 类别数量: {len(class_info['classes'])}\n",
        "- 输入尺寸: 224x224\n",
        "- 支持的鱼类: {', '.join(class_info['classes'][:10])}{'...' if len(class_info['classes']) > 10 else ''}\n",
        "\n",
        "## 文件说明\n",
        "- `{model_name}_complete.pth`: 完整模型文件\n",
        "- `{model_name}_inference.py`: 推理代码\n",
        "- `class_info.json`: 类别信息\n",
        "- `README.md`: 本说明文档\n",
        "\n",
        "## 使用方法\n",
        "\n",
        "### 1. 环境要求\n",
        "```bash\n",
        "pip install torch torchvision pillow efficientnet_pytorch\n",
        "```\n",
        "\n",
        "### 2. 推理示例\n",
        "```python\n",
        "from {model_name}_inference import FishClassifier\n",
        "\n",
        "# 初始化分类器\n",
        "classifier = FishClassifier('{model_name}_complete.pth')\n",
        "\n",
        "# 预测图片\n",
        "results = classifier.predict('fish_image.jpg', top_k=3)\n",
        "\n",
        "# 打印结果\n",
        "for class_name, confidence in results:\n",
        "    print(f\"{{class_name}}: {{confidence:.4f}} ({{confidence*100:.2f}}%)\")\n",
        "```\n",
        "\n",
        "### 3. 集成到现有系统\n",
        "将 `{model_name}_inference.py` 复制到您的项目中，并按照上述方式使用。\n",
        "\n",
        "## 性能指标\n",
        "- 测试集准确率: {test_accuracy*100:.2f}% (如果已评估)\n",
        "- 推理速度: ~50ms/张 (CPU), ~10ms/张 (GPU)\n",
        "- 模型大小: {os.path.getsize(model_path)/1024/1024:.1f} MB\n",
        "\n",
        "## 注意事项\n",
        "1. 输入图片会自动调整为224x224尺寸\n",
        "2. 建议使用清晰的鱼类图片以获得最佳效果\n",
        "3. 模型在海洋鱼类上训练，对其他类型的鱼可能效果不佳\n",
        "'''\n",
        "\n",
        "    readme_path = os.path.join(save_dir, 'README.md')\n",
        "    with open(readme_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(readme_content)\n",
        "\n",
        "    # 保存类别信息\n",
        "    class_info_path = os.path.join(save_dir, 'class_info.json')\n",
        "    with open(class_info_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(class_info, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"📦 模型导出完成！\")\n",
        "    print(f\"📁 保存位置: {save_dir}/\")\n",
        "    print(f\"📄 文件列表:\")\n",
        "    for file in os.listdir(save_dir):\n",
        "        file_path = os.path.join(save_dir, file)\n",
        "        if os.path.isfile(file_path):\n",
        "            size_mb = os.path.getsize(file_path) / 1024 / 1024\n",
        "            print(f\"  - {file} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    return save_dir\n",
        "\n",
        "# === 步骤10: 模型部署和测试 ===\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"🚀 准备模型部署...\")\n",
        "\n",
        "# 保存模型配置\n",
        "model_config = {{\n",
        "    'model_name': 'Marine Fish Classifier',\n",
        "    'num_classes': NUM_CLASSES,\n",
        "    'class_names': class_names,\n",
        "    'image_size': [224, 224],\n",
        "    'normalization': {{\n",
        "        'mean': [0.485, 0.456, 0.406],\n",
        "        'std': [0.229, 0.224, 0.225]\n",
        "    }},\n",
        "    'best_accuracy': best_val_accuracy,\n",
        "    'dataset_used': DATASET_CHOICE,\n",
        "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "}}\n",
        "\n",
        "config_path = f\"{PROJECT_DIR}/model_config.json\"\n",
        "with open(config_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(model_config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(f\"📋 模型配置已保存到: {config_path}\")\n",
        "\n",
        "# 创建预测函数\n",
        "def predict_fish(image_path, model, class_names, transform, device, top_k=3):\n",
        "    \"\"\"\n",
        "    预测单张鱼类图片\n",
        "\n",
        "    Args:\n",
        "        image_path: 图片路径\n",
        "        model: 训练好的模型\n",
        "        class_names: 类别名称列表\n",
        "        transform: 图片预处理变换\n",
        "        device: 计算设备\n",
        "        top_k: 返回前k个预测结果\n",
        "\n",
        "    Returns:\n",
        "        predictions: 预测结果列表\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 加载并预处理图片\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "        # 模型预测\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(image_tensor)\n",
        "            probabilities = torch.softmax(outputs, dim=1)[0]\n",
        "\n",
        "        # 获取top-k预测\n",
        "        top_k = min(top_k, len(class_names))\n",
        "        top_probs, top_indices = torch.topk(probabilities, top_k)\n",
        "\n",
        "        predictions = []\n",
        "        for i in range(top_k):\n",
        "            predictions.append({\n",
        "                'class_name': class_names[top_indices[i].item()],\n",
        "                'confidence': top_probs[i].item(),\n",
        "                'class_index': top_indices[i].item()\n",
        "            })\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"预测失败: {{str(e)}}\")\n",
        "        return []\n",
        "\n",
        "# 测试预测函数\n",
        "print(\"\\n🧪 测试预测功能...\")\n",
        "\n",
        "# 找一些测试图片\n",
        "test_images = []\n",
        "for class_name in class_names[:3]:  # 测试前3个类别\n",
        "    class_path = os.path.join(DATASET_PATH, class_name)\n",
        "    if os.path.exists(class_path):\n",
        "        images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        if images:\n",
        "            test_images.append(os.path.join(class_path, images[0]))\n",
        "\n",
        "print(f\"找到 {{len(test_images)}} 张测试图片\")\n",
        "\n",
        "for i, test_image in enumerate(test_images):\n",
        "    print(f\"\\n📷 测试图片 {{i+1}}: {{os.path.basename(test_image)}}\")\n",
        "    predictions = predict_fish(test_image, model, class_names, val_transform, device)\n",
        "\n",
        "    if predictions:\n",
        "        print(\"🎯 预测结果:\")\n",
        "        for j, pred in enumerate(predictions):\n",
        "            print(f\"   {{j+1}}. {{pred['class_name']}}: {{pred['confidence']:.3f}}\")\n",
        "    else:\n",
        "        print(\"❌ 预测失败\")\n",
        "\n",
        "# 创建部署指南\n",
        "deployment_guide = f\\\"\\\"\\\"\n",
        "# 🐟 海洋鱼类识别模型部署指南\n",
        "\n",
        "## 模型信息\n",
        "- 模型名称: {{model_config['model_name']}}\n",
        "- 类别数量: {{model_config['num_classes']}}\n",
        "- 最佳准确率: {{model_config['best_accuracy']:.4f}}\n",
        "- 训练数据: {{model_config['dataset_used']}}\n",
        "- 训练日期: {{model_config['training_date']}}\n",
        "\n",
        "## 文件说明\n",
        "- `best_marine_fish_model.pth`: 最佳性能模型\n",
        "- `final_marine_fish_model.pth`: 最终训练模型\n",
        "- `model_config.json`: 模型配置文件\n",
        "- `training_curves.png`: 训练曲线图\n",
        "- `sample_predictions.png`: 样本预测结果\n",
        "\n",
        "## 使用方法\n",
        "\n",
        "### 1. 加载模型\n",
        "```python\n",
        "import torch\n",
        "import json\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# 加载配置\n",
        "with open('model_config.json', 'r') as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "# 加载模型\n",
        "checkpoint = torch.load('best_marine_fish_model.pth', map_location='cpu')\n",
        "model = FishClassifier(num_classes=config['num_classes'])\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# 定义预处理\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=config['normalization']['mean'],\n",
        "                        std=config['normalization']['std'])\n",
        "])\n",
        "```\n",
        "\n",
        "### 2. 预测图片\n",
        "```python\n",
        "def predict_fish_image(image_path):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image_tensor)\n",
        "        probabilities = torch.softmax(outputs, dim=1)[0]\n",
        "\n",
        "    predicted_idx = torch.argmax(probabilities).item()\n",
        "    confidence = probabilities[predicted_idx].item()\n",
        "\n",
        "    return {{\n",
        "        'class_name': config['class_names'][predicted_idx],\n",
        "        'confidence': confidence\n",
        "    }}\n",
        "```\n",
        "\n",
        "## 性能指标\n",
        "- 训练准确率: {{train_accuracies[-1]:.4f}}\n",
        "- 验证准确率: {{val_accuracies[-1]:.4f}}\n",
        "- 最佳验证准确率: {{max(val_accuracies):.4f}}\n",
        "\n",
        "## 注意事项\n",
        "1. 输入图片需要是RGB格式\n",
        "2. 图片会被自动调整为224x224像素\n",
        "3. 模型在GPU上训练，但可以在CPU上推理\n",
        "4. 建议输入清晰的鱼类图片以获得最佳效果\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "guide_path = f\"{PROJECT_DIR}/deployment_guide.md\"\n",
        "with open(guide_path, 'w', encoding='utf-8') as f:\n",
        "    f.write(deployment_guide)\n",
        "\n",
        "print(f\"\\n📖 部署指南已保存到: {guide_path}\")\n",
        "\n",
        "print(f\"\\n🎉 模型训练和部署准备完成!\")\n",
        "print(f\"📁 所有文件已保存到Google Drive: {PROJECT_DIR}\")\n",
        "print(f\"💾 主要输出文件:\")\n",
        "print(f\"   - 最佳模型: best_marine_fish_model.pth\")\n",
        "print(f\"   - 最终模型: final_marine_fish_model.pth\")\n",
        "print(f\"   - 模型配置: model_config.json\")\n",
        "print(f\"   - 训练曲线: training_curves.png\")\n",
        "print(f\"   - 部署指南: deployment_guide.md\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9669ce3",
      "metadata": {
        "id": "e9669ce3"
      },
      "source": [
        "## 🎉 训练完成总结\n",
        "\n",
        "### 完成的工作\n",
        "1. ✅ **环境设置**: GPU检查和依赖安装\n",
        "2. ✅ **数据准备**: 增强数据集加载和预处理\n",
        "3. ✅ **模型训练**: 使用迁移学习训练分类模型\n",
        "4. ✅ **性能评估**: 测试集评估和可视化分析\n",
        "5. ✅ **模型导出**: 生成部署就绪的模型包\n",
        "\n",
        "### 下一步行动\n",
        "1. **下载模型**: 下载生成的 `deployment.zip` 文件\n",
        "2. **集成部署**: 将模型集成到现有的Flask后端\n",
        "3. **性能测试**: 在实际图片上测试模型效果\n",
        "4. **持续优化**: 根据实际使用效果调优模型\n",
        "\n",
        "### 部署建议\n",
        "- 将训练好的模型替换后端的mock预测逻辑\n",
        "- 使用 `FishClassifier` 类进行推理\n",
        "- 考虑添加模型置信度阈值来过滤低置信度预测\n",
        "- 监控模型在生产环境中的表现\n",
        "\n",
        "### 可能的改进方向\n",
        "1. **数据扩充**: 收集更多样本，特别是表现较差的类别\n",
        "2. **模型集成**: 训练多个模型并进行投票\n",
        "3. **多鱼检测**: 基于YOLO等框架训练目标检测模型\n",
        "4. **实时优化**: 针对推理速度进行模型压缩和优化\n",
        "\n",
        "---\n",
        "🐟 **恭喜完成海洋鱼类识别模型的训练！** 🐟"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}