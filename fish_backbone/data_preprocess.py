# æµ·æ´‹é±¼ç±»æ•°æ®é›†é¢„å¤„ç†å’Œå‡è¡¡åŒ–
# å®ç°æ•°æ®å‡è¡¡ã€åˆ’åˆ†å’Œé¢„å¤„ç†åŠŸèƒ½

import os
import shutil
import random
import glob
from collections import Counter
import json

def analyze_dataset(dataset_root):
    """
    åˆ†æåŸå§‹æ•°æ®é›†çš„åˆ†å¸ƒæƒ…å†µ
    """
    print("ğŸ” åˆ†æåŸå§‹æ•°æ®é›†...")
    
    classes = [d for d in os.listdir(dataset_root) 
               if os.path.isdir(os.path.join(dataset_root, d)) and not d.startswith('.')]
    classes.sort()
    
    class_stats = {}
    total_images = 0
    
    for cls in classes:
        cls_dir = os.path.join(dataset_root, cls)
        images = glob.glob(os.path.join(cls_dir, "*.png")) + \
                glob.glob(os.path.join(cls_dir, "*.jpg")) + \
                glob.glob(os.path.join(cls_dir, "*.jpeg"))
        
        class_stats[cls] = len(images)
        total_images += len(images)
    
    # ç»Ÿè®¡åˆ†æ
    counts = list(class_stats.values())
    min_count = min(counts)
    max_count = max(counts)
    avg_count = sum(counts) / len(counts)
    
    print(f"\nğŸ“Š æ•°æ®é›†åˆ†æç»“æœ:")
    print(f"  æ€»ç±»åˆ«æ•°: {len(classes)}")
    print(f"  æ€»å›¾ç‰‡æ•°: {total_images:,}")
    print(f"  å¹³å‡æ¯ç±»: {avg_count:.1f} å¼ ")
    print(f"  æœ€å°‘æ ·æœ¬: {min_count} å¼  ({min(class_stats, key=class_stats.get)})")
    print(f"  æœ€å¤šæ ·æœ¬: {max_count:,} å¼  ({max(class_stats, key=class_stats.get)})")
    print(f"  æ•°æ®ä¸å‡è¡¡æ¯”: {min_count/max_count:.4f}")
    
    # è¯¦ç»†åˆ†å¸ƒ
    print(f"\nğŸ“ˆ å„ç±»åˆ«æ ·æœ¬æ•°é‡:")
    for i, (cls, count) in enumerate(sorted(class_stats.items(), key=lambda x: x[1], reverse=True)):
        status = ""
        if count > 200:
            status = "ğŸ“‰ éœ€è¦æŠ½æ ·"
        elif count < 50:
            status = "âš ï¸  æ ·æœ¬è¾ƒå°‘" 
        elif count < 100:
            status = "ğŸŸ¡ ä¸­ç­‰æ ·æœ¬"
        else:
            status = "âœ… æ ·æœ¬å……è¶³"
            
        print(f"  {i+1:2d}. {cls:<30} {count:4d} å¼  {status}")
    
    return classes, class_stats

def balance_dataset(original_root, balanced_root, max_samples_per_class=200, min_samples_per_class=20):
    """
    å‡è¡¡åŒ–æ•°æ®é›† - è§£å†³æ ·æœ¬ä¸å‡è¡¡é—®é¢˜
    """
    print(f"\nğŸ¯ å¼€å§‹æ•°æ®å‡è¡¡åŒ–å¤„ç†...")
    print(f"  ç­–ç•¥: æ¯ç±»æœ€å¤šä¿ç•™ {max_samples_per_class} å¼ å›¾ç‰‡")
    print(f"  è¿‡æ»¤: å°‘äº {min_samples_per_class} å¼ çš„ç±»åˆ«å°†è¢«æ’é™¤")
    
    # åˆ›å»ºå‡è¡¡æ•°æ®é›†ç›®å½•
    if os.path.exists(balanced_root):
        print(f"  æ¸…ç†æ—§çš„å‡è¡¡æ•°æ®é›†: {balanced_root}")
        shutil.rmtree(balanced_root)
    os.makedirs(balanced_root)
    
    # è·å–åŸå§‹æ•°æ®ç»Ÿè®¡
    classes, class_stats = analyze_dataset(original_root)
    
    # è¿‡æ»¤å’Œå¤„ç†æ¯ä¸ªç±»åˆ«
    processed_stats = {}
    excluded_classes = []
    
    for cls in classes:
        original_count = class_stats[cls]
        
        # è·³è¿‡æ ·æœ¬è¿‡å°‘çš„ç±»åˆ«
        if original_count < min_samples_per_class:
            excluded_classes.append((cls, original_count))
            print(f"  ğŸš« æ’é™¤ {cls}: æ ·æœ¬æ•° {original_count} < {min_samples_per_class}")
            continue
        
        # åˆ›å»ºç±»åˆ«ç›®å½•
        cls_balanced_dir = os.path.join(balanced_root, cls)
        os.makedirs(cls_balanced_dir)
        
        # è·å–è¯¥ç±»åˆ«çš„æ‰€æœ‰å›¾ç‰‡
        cls_original_dir = os.path.join(original_root, cls)
        all_images = glob.glob(os.path.join(cls_original_dir, "*.png")) + \
                    glob.glob(os.path.join(cls_original_dir, "*.jpg")) + \
                    glob.glob(os.path.join(cls_original_dir, "*.jpeg"))
        
        # éšæœºæŠ½æ ·æˆ–å…¨éƒ¨ä¿ç•™
        if len(all_images) > max_samples_per_class:
            # éšæœºæŠ½æ ·
            random.shuffle(all_images)
            selected_images = all_images[:max_samples_per_class]
            action = f"æŠ½æ · {len(selected_images)}/{len(all_images)}"
        else:
            # å…¨éƒ¨ä¿ç•™
            selected_images = all_images
            action = f"å…¨ä¿ç•™ {len(selected_images)}"
        
        # å¤åˆ¶é€‰ä¸­çš„å›¾ç‰‡
        for i, img_path in enumerate(selected_images):
            filename = os.path.basename(img_path)
            # é‡å‘½åä»¥é¿å…å†²çª
            name, ext = os.path.splitext(filename)
            new_filename = f"{cls}_{i:04d}{ext}"
            new_path = os.path.join(cls_balanced_dir, new_filename)
            shutil.copy2(img_path, new_path)
        
        processed_stats[cls] = len(selected_images)
        print(f"  âœ… {cls:<30} {action}")
    
    # è¾“å‡ºå¤„ç†ç»“æœ
    print(f"\nâœ… æ•°æ®å‡è¡¡åŒ–å®Œæˆ!")
    print(f"  ä¿ç•™ç±»åˆ«: {len(processed_stats)} ä¸ª")
    print(f"  æ’é™¤ç±»åˆ«: {len(excluded_classes)} ä¸ª")
    
    if excluded_classes:
        print(f"\n  è¢«æ’é™¤çš„ç±»åˆ«:")
        for cls, count in excluded_classes:
            print(f"    - {cls}: {count} å¼ ")
    
    # æ–°æ•°æ®é›†ç»Ÿè®¡
    total_balanced = sum(processed_stats.values())
    avg_balanced = total_balanced / len(processed_stats)
    min_balanced = min(processed_stats.values())
    max_balanced = max(processed_stats.values())
    
    print(f"\nğŸ“Š å‡è¡¡åæ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ€»ç±»åˆ«æ•°: {len(processed_stats)}")
    print(f"  æ€»å›¾ç‰‡æ•°: {total_balanced:,}")
    print(f"  å¹³å‡æ¯ç±»: {avg_balanced:.1f} å¼ ")
    print(f"  æ ·æœ¬èŒƒå›´: {min_balanced} - {max_balanced} å¼ ")
    print(f"  æ–°å‡è¡¡æ¯”: {min_balanced/max_balanced:.4f}")
    
    return processed_stats

def split_balanced_dataset(balanced_root, output_root, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """
    å°†å‡è¡¡åŒ–çš„æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    """
    print(f"\nğŸ“‚ åˆ’åˆ†æ•°æ®é›†...")
    print(f"  æ¯”ä¾‹ - è®­ç»ƒ:{train_ratio:.0%} éªŒè¯:{val_ratio:.0%} æµ‹è¯•:{test_ratio:.0%}")
    
    # éªŒè¯æ¯”ä¾‹
    if abs(train_ratio + val_ratio + test_ratio - 1.0) > 0.001:
        raise ValueError("è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ç­‰äº1")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
    splits = ['train', 'val', 'test']
    for split in splits:
        split_dir = os.path.join(output_root, split)
        if os.path.exists(split_dir):
            shutil.rmtree(split_dir)
        os.makedirs(split_dir)
    
    # è·å–æ‰€æœ‰ç±»åˆ«
    classes = [d for d in os.listdir(balanced_root) 
               if os.path.isdir(os.path.join(balanced_root, d))]
    
    split_stats = {'train': {}, 'val': {}, 'test': {}}
    
    for cls in classes:
        cls_dir = os.path.join(balanced_root, cls)
        all_images = glob.glob(os.path.join(cls_dir, "*.png")) + \
                    glob.glob(os.path.join(cls_dir, "*.jpg")) + \
                    glob.glob(os.path.join(cls_dir, "*.jpeg"))
        
        # éšæœºæ‰“æ•£
        random.shuffle(all_images)
        
        total_count = len(all_images)
        train_count = int(total_count * train_ratio)
        val_count = int(total_count * val_ratio)
        # test_count = total_count - train_count - val_count  # å‰©ä½™çš„éƒ½ç»™æµ‹è¯•é›†
        
        # åˆ’åˆ†æ•°æ®
        splits_data = {
            'train': all_images[:train_count],
            'val': all_images[train_count:train_count + val_count],
            'test': all_images[train_count + val_count:]
        }
        
        # åˆ›å»ºå„splitçš„ç±»åˆ«ç›®å½•å¹¶å¤åˆ¶æ–‡ä»¶
        for split, images in splits_data.items():
            split_cls_dir = os.path.join(output_root, split, cls)
            os.makedirs(split_cls_dir, exist_ok=True)
            
            for img_path in images:
                filename = os.path.basename(img_path)
                shutil.copy2(img_path, os.path.join(split_cls_dir, filename))
            
            split_stats[split][cls] = len(images)
        
        print(f"  {cls:<30} è®­ç»ƒ:{len(splits_data['train']):3d} éªŒè¯:{len(splits_data['val']):3d} æµ‹è¯•:{len(splits_data['test']):3d}")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nâœ… æ•°æ®é›†åˆ’åˆ†å®Œæˆ!")
    for split in splits:
        total = sum(split_stats[split].values())
        print(f"  {split.upper():>5}é›†: {total:4d} å¼ å›¾ç‰‡, {len(split_stats[split])} ä¸ªç±»åˆ«")
    
    return split_stats

def generate_paddlex_format(dataset_root, output_dir):
    """
    ç”ŸæˆPaddleXè®­ç»ƒæ‰€éœ€çš„æ ¼å¼æ–‡ä»¶
    """
    print(f"\nğŸ“ ç”ŸæˆPaddleXæ ¼å¼æ–‡ä»¶...")
    
    # è·å–ç±»åˆ«åˆ—è¡¨
    train_dir = os.path.join(dataset_root, 'train')
    classes = sorted([d for d in os.listdir(train_dir) 
                     if os.path.isdir(os.path.join(train_dir, d))])
    
    print(f"  å‘ç° {len(classes)} ä¸ªç±»åˆ«")
    
    # åˆ›å»ºæ ‡ç­¾æ˜ å°„
    label_map = {cls: i for i, cls in enumerate(classes)}
    
    # ç”Ÿæˆå„ç§æ ¼å¼æ–‡ä»¶
    splits = ['train', 'val', 'test']
    
    for split in splits:
        split_dir = os.path.join(dataset_root, split)
        if not os.path.exists(split_dir):
            continue
            
        data_list = []
        
        for cls in classes:
            cls_dir = os.path.join(split_dir, cls)
            if not os.path.exists(cls_dir):
                continue
                
            images = glob.glob(os.path.join(cls_dir, "*.png")) + \
                    glob.glob(os.path.join(cls_dir, "*.jpg")) + \
                    glob.glob(os.path.join(cls_dir, "*.jpeg"))
            
            for img_path in images:
                # ç›¸å¯¹äºdataset_rootçš„è·¯å¾„
                rel_path = os.path.relpath(img_path, dataset_root)
                data_list.append(f"{rel_path}\t{label_map[cls]}")
        
        # ä¿å­˜åˆ—è¡¨æ–‡ä»¶
        list_file = os.path.join(output_dir, f"{split}_list.txt")
        with open(list_file, 'w', encoding='utf-8') as f:
            for item in data_list:
                f.write(f"{item}\n")
        
        print(f"  âœ… {split}_list.txt: {len(data_list)} æ¡è®°å½•")
    
    # ä¿å­˜æ ‡ç­¾æ–‡ä»¶
    label_file = os.path.join(output_dir, "label_list.txt")
    with open(label_file, 'w', encoding='utf-8') as f:
        for cls in classes:
            f.write(f"{cls}\n")
    
    # ä¿å­˜ç±»åˆ«åç§°æ–‡ä»¶
    class_names_file = os.path.join(output_dir, "class_names.txt")
    with open(class_names_file, 'w', encoding='utf-8') as f:
        for cls in classes:
            f.write(f"{cls}\n")
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config = {
        "num_classes": len(classes),
        "classes": classes,
        "label_map": label_map,
        "dataset_root": dataset_root
    }
    
    config_file = os.path.join(output_dir, "dataset_config.json")
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ… label_list.txt: {len(classes)} ä¸ªç±»åˆ«")
    print(f"  âœ… dataset_config.json: é…ç½®ä¿¡æ¯")
    
    return classes, label_map

def main():
    """
    ä¸»å‡½æ•° - å®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹
    """
    print("ğŸŸ æµ·æ´‹é±¼ç±»æ•°æ®é›†é¢„å¤„ç†ç³»ç»Ÿ")
    print("=" * 60)
    
    # è·¯å¾„é…ç½®  
    original_dataset = "../../dataset"                       # åŸå§‹æ•°æ®é›†
    balanced_dataset = "../../dataset_balanced"              # å‡è¡¡åæ•°æ®é›†  
    final_dataset = "../../dataset_processed"               # æœ€ç»ˆå¤„ç†åçš„æ•°æ®é›†
    config_output = "./"                                     # é…ç½®æ–‡ä»¶è¾“å‡ºç›®å½•
    
    # æ£€æŸ¥åŸå§‹æ•°æ®é›†
    if not os.path.exists(original_dataset):
        print(f"âŒ åŸå§‹æ•°æ®é›†ä¸å­˜åœ¨: {original_dataset}")
        return
    
    try:
        # æ­¥éª¤1: åˆ†æåŸå§‹æ•°æ®é›†
        print("\n" + "="*60)
        classes, class_stats = analyze_dataset(original_dataset)
        
        # æ­¥éª¤2: æ•°æ®å‡è¡¡åŒ–
        print("\n" + "="*60)
        balanced_stats = balance_dataset(
            original_dataset, 
            balanced_dataset,
            max_samples_per_class=200,  # æ¯ç±»æœ€å¤š200å¼ 
            min_samples_per_class=20    # æ¯ç±»è‡³å°‘20å¼ 
        )
        
        # æ­¥éª¤3: åˆ’åˆ†æ•°æ®é›†
        print("\n" + "="*60)
        split_stats = split_balanced_dataset(
            balanced_dataset,
            final_dataset,
            train_ratio=0.7,    # 70%è®­ç»ƒ
            val_ratio=0.15,     # 15%éªŒè¯  
            test_ratio=0.15     # 15%æµ‹è¯•
        )
        
        # æ­¥éª¤4: ç”ŸæˆPaddleXæ ¼å¼æ–‡ä»¶
        print("\n" + "="*60)
        classes, label_map = generate_paddlex_format(final_dataset, config_output)
        
        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ‰ æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶å’Œç›®å½•:")
        print(f"  åŸå§‹æ•°æ®é›†:     {original_dataset}")
        print(f"  å‡è¡¡æ•°æ®é›†:     {balanced_dataset}")
        print(f"  æœ€ç»ˆæ•°æ®é›†:     {final_dataset}")
        print(f"    â”œâ”€â”€ train/   (è®­ç»ƒé›†)")
        print(f"    â”œâ”€â”€ val/     (éªŒè¯é›†)")
        print(f"    â””â”€â”€ test/    (æµ‹è¯•é›†)")
        print(f"\nğŸ“ è®­ç»ƒé…ç½®æ–‡ä»¶:")
        print(f"  â”œâ”€â”€ train_list.txt      (è®­ç»ƒæ•°æ®åˆ—è¡¨)")
        print(f"  â”œâ”€â”€ val_list.txt        (éªŒè¯æ•°æ®åˆ—è¡¨)")
        print(f"  â”œâ”€â”€ test_list.txt       (æµ‹è¯•æ•°æ®åˆ—è¡¨)")
        print(f"  â”œâ”€â”€ label_list.txt      (ç±»åˆ«æ ‡ç­¾)")
        print(f"  â””â”€â”€ dataset_config.json (æ•°æ®é›†é…ç½®)")
        
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        train_total = sum(split_stats['train'].values())
        val_total = sum(split_stats['val'].values())
        test_total = sum(split_stats['test'].values())
        total = train_total + val_total + test_total
        
        print(f"  ç±»åˆ«æ•°é‡: {len(classes)}")
        print(f"  è®­ç»ƒæ ·æœ¬: {train_total:,} å¼  ({train_total/total:.1%})")
        print(f"  éªŒè¯æ ·æœ¬: {val_total:,} å¼  ({val_total/total:.1%})")
        print(f"  æµ‹è¯•æ ·æœ¬: {test_total:,} å¼  ({test_total/total:.1%})")
        print(f"  æ€»æ ·æœ¬æ•°: {total:,} å¼ ")
        
        print(f"\nğŸš€ ä¸‹ä¸€æ­¥:")
        print("  1. æ£€æŸ¥ç”Ÿæˆçš„æ•°æ®é›†ç»“æ„")
        print("  2. è¿è¡Œ 'python train_resnet_model.py' å¼€å§‹è®­ç»ƒ")
        print("  3. ä½¿ç”¨ResNet50 + è¿ç§»å­¦ä¹ è®­ç»ƒé±¼ç±»åˆ†ç±»æ¨¡å‹")
        
    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # è®¾ç½®éšæœºç§å­ä»¥ä¿è¯ç»“æœå¯å¤ç°
    random.seed(42)
    main()
