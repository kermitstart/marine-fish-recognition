# å¿«é€Ÿæµ‹è¯•è®­ç»ƒè„šæœ¬ - éªŒè¯ä»£ç å¯è¡Œæ€§
# ä½¿ç”¨å°æ•°æ®é›†å’Œå°‘é‡epochè¿›è¡Œæµ‹è¯•

import paddlex as pdx
import paddle
import os
import json
import time
import glob
import random
import shutil

def create_mini_dataset(source_dir="../../dataset", target_dir="./mini_dataset", samples_per_class=10):
    """
    åˆ›å»ºå°å‹æµ‹è¯•æ•°æ®é›†
    """
    print(f"ğŸ”„ åˆ›å»ºå°å‹æµ‹è¯•æ•°æ®é›†...")
    print(f"  æºç›®å½•: {source_dir}")
    print(f"  ç›®æ ‡ç›®å½•: {target_dir}")
    print(f"  æ¯ç±»æ ·æœ¬æ•°: {samples_per_class}")
    
    # æ¸…ç†ç›®æ ‡ç›®å½•
    if os.path.exists(target_dir):
        shutil.rmtree(target_dir)
    
    # åˆ›å»ºç›®å½•ç»“æ„
    train_dir = os.path.join(target_dir, "train")
    val_dir = os.path.join(target_dir, "val")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(val_dir, exist_ok=True)
    
    # è·å–å‰5ä¸ªç±»åˆ«è¿›è¡Œæµ‹è¯•
    if not os.path.exists(source_dir):
        print(f"âŒ æ•°æ®é›†ç›®å½•ä¸å­˜åœ¨: {source_dir}")
        return None, None
    
    all_classes = [d for d in os.listdir(source_dir) 
                   if os.path.isdir(os.path.join(source_dir, d)) and not d.startswith('.')]
    all_classes.sort()
    
    # é€‰æ‹©å‰5ä¸ªç±»åˆ«è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    test_classes = all_classes[:5]
    print(f"  æµ‹è¯•ç±»åˆ«: {test_classes}")
    
    train_files = []
    val_files = []
    class_stats = {}
    
    for i, class_name in enumerate(test_classes):
        source_class_dir = os.path.join(source_dir, class_name)
        
        # è·å–è¯¥ç±»åˆ«çš„æ‰€æœ‰å›¾ç‰‡
        images = glob.glob(os.path.join(source_class_dir, "*.png")) + \
                glob.glob(os.path.join(source_class_dir, "*.jpg")) + \
                glob.glob(os.path.join(source_class_dir, "*.jpeg"))
        
        if len(images) == 0:
            print(f"  âš ï¸  {class_name} ç±»åˆ«æ²¡æœ‰å›¾ç‰‡")
            continue
        
        # éšæœºé€‰æ‹©æ ·æœ¬
        random.shuffle(images)
        selected_images = images[:samples_per_class]
        
        # 80%è®­ç»ƒï¼Œ20%éªŒè¯
        split_idx = max(1, int(len(selected_images) * 0.8))
        train_images = selected_images[:split_idx]
        val_images = selected_images[split_idx:]
        
        # åˆ›å»ºç±»åˆ«ç›®å½•
        train_class_dir = os.path.join(train_dir, class_name)
        val_class_dir = os.path.join(val_dir, class_name)
        os.makedirs(train_class_dir, exist_ok=True)
        os.makedirs(val_class_dir, exist_ok=True)
        
        # å¤åˆ¶è®­ç»ƒå›¾ç‰‡
        for img_path in train_images:
            filename = os.path.basename(img_path)
            target_path = os.path.join(train_class_dir, filename)
            shutil.copy2(img_path, target_path)
            
            rel_path = os.path.join("train", class_name, filename)
            train_files.append(f"{rel_path}\t{i}")
        
        # å¤åˆ¶éªŒè¯å›¾ç‰‡
        for img_path in val_images:
            filename = os.path.basename(img_path)
            target_path = os.path.join(val_class_dir, filename)
            shutil.copy2(img_path, target_path)
            
            rel_path = os.path.join("val", class_name, filename)
            val_files.append(f"{rel_path}\t{i}")
        
        class_stats[class_name] = {
            'train': len(train_images),
            'val': len(val_images),
            'total': len(selected_images)
        }
        
        print(f"    {class_name}: è®­ç»ƒ{len(train_images)}å¼ , éªŒè¯{len(val_images)}å¼ ")
    
    # ä¿å­˜æ•°æ®åˆ—è¡¨æ–‡ä»¶
    with open("mini_train_list.txt", "w", encoding="utf-8") as f:
        for item in train_files:
            f.write(f"{item}\n")
    
    with open("mini_val_list.txt", "w", encoding="utf-8") as f:
        for item in val_files:
            f.write(f"{item}\n")
    
    # ä¿å­˜ç±»åˆ«åˆ—è¡¨
    with open("mini_class_names.txt", "w", encoding="utf-8") as f:
        for cls in test_classes:
            f.write(f"{cls}\n")
    
    # ä¿å­˜é…ç½®
    config = {
        "dataset_root": target_dir,
        "num_classes": len(test_classes),
        "classes": test_classes,
        "train_samples": len(train_files),
        "val_samples": len(val_files),
        "class_stats": class_stats
    }
    
    with open("mini_dataset_config.json", "w", encoding="utf-8") as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… å°å‹æ•°æ®é›†åˆ›å»ºå®Œæˆ:")
    print(f"  ç±»åˆ«æ•°: {len(test_classes)}")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_files)}")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_files)}")
    
    return test_classes, config

def quick_test_training():
    """
    å¿«é€Ÿæµ‹è¯•è®­ç»ƒ
    """
    print("ğŸ§ª å¼€å§‹å¿«é€Ÿæµ‹è¯•è®­ç»ƒ...")
    
    # æ£€æŸ¥ç¯å¢ƒ
    print(f"  PaddleXç‰ˆæœ¬: {pdx.__version__}")
    print(f"  Paddleç‰ˆæœ¬: {paddle.__version__}")
    
    device = "gpu" if paddle.is_compiled_with_cuda() else "cpu"
    print(f"  è®¾å¤‡: {device}")
    
    try:
        # åˆ›å»ºå›¾åƒåˆ†ç±»ç®¡é“è¿›è¡Œæµ‹è¯•
        print("ğŸ¤– åˆ›å»ºåˆ†ç±»ç®¡é“...")
        pipeline = pdx.create_pipeline(
            pipeline="image_classification",
            device=device
        )
        print("âœ… åˆ†ç±»ç®¡é“åˆ›å»ºæˆåŠŸ")
        
        # è®­ç»ƒé…ç½® - æå°è§„æ¨¡æµ‹è¯•
        train_config = {
            "epochs": 3,       # åªè®­ç»ƒ3ä¸ªepoch
            "batch_size": 4,   # å°æ‰¹æ¬¡
            "learning_rate": 0.001,
            "save_dir": "./test_output"
        }
        
        print(f"\nğŸ“‹ æµ‹è¯•è®­ç»ƒé…ç½®:")
        for key, value in train_config.items():
            print(f"  {key}: {value}")
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\nğŸš€ å¼€å§‹æµ‹è¯•è®­ç»ƒï¼ˆé¢„è®¡1-3åˆ†é’Ÿï¼‰...")
        start_time = time.time()
        
        result = pipeline.train(
            train_dataset="mini_train_list.txt",
            eval_dataset="mini_val_list.txt",
            **train_config
        )
        
        training_time = time.time() - start_time
        print(f"\nâœ… æµ‹è¯•è®­ç»ƒå®Œæˆ!")
        print(f"  è®­ç»ƒæ—¶é—´: {training_time:.1f} ç§’")
        print(f"  è®­ç»ƒç»“æœ: {result}")
        
        # æµ‹è¯•æ¨ç†
        print(f"\nğŸ” æµ‹è¯•æ¨¡å‹æ¨ç†...")
        
        # æ‰¾ä¸€å¼ æµ‹è¯•å›¾ç‰‡
        test_image = None
        for root, dirs, files in os.walk("./mini_dataset/val"):
            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    test_image = os.path.join(root, file)
                    break
            if test_image:
                break
        
        if test_image:
            print(f"  æµ‹è¯•å›¾ç‰‡: {test_image}")
            pred_result = pipeline.predict([test_image])
            print(f"  é¢„æµ‹ç»“æœ: {pred_result}")
        
        # å¯¼å‡ºæ¨¡å‹
        print(f"\nğŸ’¾ å¯¼å‡ºæµ‹è¯•æ¨¡å‹...")
        pipeline.export(save_dir="./test_inference_model")
        print("âœ… æ¨¡å‹å¯¼å‡ºæˆåŠŸ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """
    ä¸»æµ‹è¯•æµç¨‹
    """
    print("ğŸ§ª å¿«é€Ÿæµ‹è¯•è®­ç»ƒç³»ç»Ÿ")
    print("=" * 50)
    
    try:
        # 1. åˆ›å»ºå°å‹æ•°æ®é›†
        test_classes, config = create_mini_dataset()
        
        if test_classes is None:
            print("âŒ å°å‹æ•°æ®é›†åˆ›å»ºå¤±è´¥")
            return
        
        # 2. è¿›è¡Œå¿«é€Ÿè®­ç»ƒæµ‹è¯•
        print("\n" + "=" * 50)
        success = quick_test_training()
        
        if success:
            print("\n" + "=" * 50)
            print("ğŸ‰ å¿«é€Ÿæµ‹è¯•æˆåŠŸ!")
            print("\nğŸ“‹ æµ‹è¯•ç»“æœ:")
            print("âœ… ä»£ç è¿è¡Œæ­£å¸¸")
            print("âœ… æ¨¡å‹è®­ç»ƒæˆåŠŸ") 
            print("âœ… æ¨¡å‹æ¨ç†æ­£å¸¸")
            print("âœ… æ¨¡å‹å¯¼å‡ºæˆåŠŸ")
            
            print(f"\nğŸš€ å¯ä»¥è¿›è¡Œå®Œæ•´è®­ç»ƒäº†!")
            print(f"å»ºè®®åœ¨Colabä¸­:")
            print(f"1. ä¸Šä¼ å®Œæ•´æ•°æ®é›†")
            print(f"2. ä½¿ç”¨GPUè®­ç»ƒ")
            print(f"3. å¢åŠ è®­ç»ƒè½®æ•°åˆ°30-50")
            print(f"4. ä½¿ç”¨å®Œæ•´çš„23ä¸ªç±»åˆ«")
            
        else:
            print("\nâŒ å¿«é€Ÿæµ‹è¯•å¤±è´¥")
            print("éœ€è¦ä¿®å¤é”™è¯¯åå†è¿›è¡Œå®Œæ•´è®­ç»ƒ")
    
    except KeyboardInterrupt:
        print("\nâš ï¸  æµ‹è¯•è¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
