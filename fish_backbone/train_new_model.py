# ä½¿ç”¨æ–°ç‰ˆæœ¬PaddleXé‡æ–°è®­ç»ƒé±¼ç±»åˆ†ç±»æ¨¡å‹

import paddlex as pdx
import os
import json

def create_dataset_config():
    """
    åˆ›å»ºPaddleX 3.xå…¼å®¹çš„æ•°æ®é›†é…ç½®
    """
    config = {
        "Global": {
            "dataset_dir": "../../../dataset",
            "device": "gpu"  # å¦‚æœæœ‰GPUçš„è¯ï¼Œå¦åˆ™æ”¹ä¸º"cpu"
        },
        "Train": {
            "dataset": {
                "name": "ClsDataset", 
                "data_root": "../../../dataset",
                "train_list": "train_list.txt"
            },
            "dataloader": {
                "batch_size": 16,
                "num_workers": 4,
                "shuffle": True
            }
        },
        "Eval": {
            "dataset": {
                "name": "ClsDataset",
                "data_root": "../../../dataset", 
                "val_list": "val_list.txt"
            },
            "dataloader": {
                "batch_size": 16,
                "num_workers": 4,
                "shuffle": False
            }
        }
    }
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    with open("dataset_config.yaml", "w", encoding="utf-8") as f:
        import yaml
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
    
    return config

def train_fish_classification_model():
    """
    ä½¿ç”¨æ–°ç‰ˆæœ¬PaddleXè®­ç»ƒé±¼ç±»åˆ†ç±»æ¨¡å‹
    """
    
    print("ğŸ”„ å‡†å¤‡æ•°æ®é›†...")
    
    # 1. å‡†å¤‡æ•°æ®é›†é…ç½®
    dataset_config = create_dataset_config()
    
    try:
        # 2. åˆ›å»ºåˆ†ç±»ç®¡é“ - ä½¿ç”¨PaddleX 3.x API
        print("ğŸ¤– åˆ›å»ºåˆ†ç±»ç®¡é“...")
        
        # å°è¯•ä½¿ç”¨å›¾åƒåˆ†ç±»ç®¡é“
        pipeline = pdx.create_pipeline(
            pipeline="image_classification",
            llm_name=None,  # ä¸ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹
            device="gpu"    # ä½¿ç”¨CPUï¼Œå¦‚æœæœ‰GPUå¯ä»¥æ”¹ä¸º"gpu"
        )
        
        # 3. è®­ç»ƒæ¨¡å‹
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # åˆ›å»ºè®­ç»ƒé…ç½®
        train_config = {
            "epochs": 30,           # è®­ç»ƒè½®æ•°
            "learning_rate": 0.001, # å­¦ä¹ ç‡  
            "batch_size": 16,       # æ‰¹æ¬¡å¤§å°
            "save_dir": "./output_new"  # ä¿å­˜è·¯å¾„
        }
        
        # å¼€å§‹è®­ç»ƒ
        pipeline.train(
            train_dataset="train_list.txt",
            eval_dataset="val_list.txt", 
            **train_config
        )
        
        # 4. è¯„ä¼°æ¨¡å‹
        print("ğŸ“Š è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        eval_result = pipeline.evaluate("val_list.txt")
        print(f"è¯„ä¼°ç»“æœ: {eval_result}")
        
        # 5. å¯¼å‡ºæ¨¡å‹
        print("ğŸ’¾ å¯¼å‡ºæ¨¡å‹...")
        pipeline.export(save_dir="./inference_model_new")
        
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        print("å°è¯•ä½¿ç”¨æ›¿ä»£è®­ç»ƒæ–¹æ³•...")
        return train_with_alternative_method()

def train_with_alternative_method():
    """
    ä½¿ç”¨æ›¿ä»£æ–¹æ³•è®­ç»ƒæ¨¡å‹
    """
    try:
        print("ï¿½ ä½¿ç”¨ç®€åŒ–è®­ç»ƒæ–¹æ³•...")
        
        # åˆ›å»ºç®€å•çš„å›¾åƒåˆ†ç±»å™¨
        model = pdx.create_model("PP-LCNet_x1_0")
        
        # ç®€åŒ–çš„è®­ç»ƒé…ç½®
        train_result = model.train(
            train_list="train_list.txt",
            eval_list="val_list.txt",
            num_classes=23,  # 23ç§é±¼ç±»
            epochs=20,
            learning_rate=0.001,
            batch_size=8,   # é™ä½æ‰¹æ¬¡å¤§å°ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
            save_dir="./output_simple"
        )
        
        print("ï¿½ è®­ç»ƒç»“æœ:", train_result)
        
        # å¯¼å‡ºæ¨¡å‹
        model.export(save_dir="./inference_model_simple")
        
        print("âœ… ç®€åŒ–è®­ç»ƒå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ç®€åŒ–è®­ç»ƒä¹Ÿå¤±è´¥äº†: {e}")
        print("å»ºè®®æ£€æŸ¥PaddleXç‰ˆæœ¬å’Œæ•°æ®é›†æ ¼å¼")
        return False

def prepare_dataset_files():
    """
    å‡†å¤‡æ•°æ®é›†é…ç½®æ–‡ä»¶ - é€‚é…PaddleXæ ¼å¼
    """
    import glob
    import random
    import shutil
    
    dataset_root = "../../../dataset"
    
    print("ğŸ” åˆ†ææ•°æ®é›†...")
    
    # è·å–æ‰€æœ‰ç±»åˆ«
    classes = [d for d in os.listdir(dataset_root) 
               if os.path.isdir(os.path.join(dataset_root, d)) and not d.startswith('.')]
    classes.sort()
    
    print(f"å‘ç° {len(classes)} ä¸ªé±¼ç±»ç±»åˆ«:")
    total_images = 0
    class_stats = {}
    
    for i, cls in enumerate(classes):
        cls_dir = os.path.join(dataset_root, cls)
        images = glob.glob(os.path.join(cls_dir, "*.png")) + \
                glob.glob(os.path.join(cls_dir, "*.jpg")) + \
                glob.glob(os.path.join(cls_dir, "*.jpeg"))
        
        class_stats[cls] = len(images)
        total_images += len(images)
        print(f"  {i:2d}: {cls:<30} - {len(images):4d} å¼ å›¾ç‰‡")
    
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ€»ç±»åˆ«æ•°: {len(classes)}")
    print(f"  æ€»å›¾ç‰‡æ•°: {total_images}")
    print(f"  å¹³å‡æ¯ç±»: {total_images // len(classes)} å¼ ")
    
    # åˆ›å»ºæ ‡ç­¾æ˜ å°„æ–‡ä»¶
    label_map = {}
    with open("label_list.txt", "w", encoding="utf-8") as f:
        for i, cls in enumerate(classes):
            f.write(f"{cls}\n")
            label_map[cls] = i
    
    # å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®åˆ—è¡¨
    train_list = []
    val_list = []
    
    print("\nğŸ”„ å‡†å¤‡è®­ç»ƒ/éªŒè¯æ•°æ®åˆ†å‰²...")
    
    for cls in classes:
        cls_dir = os.path.join(dataset_root, cls)
        images = glob.glob(os.path.join(cls_dir, "*.png")) + \
                glob.glob(os.path.join(cls_dir, "*.jpg")) + \
                glob.glob(os.path.join(cls_dir, "*.jpeg"))
        
        if len(images) == 0:
            print(f"âš ï¸  è­¦å‘Š: {cls} ç±»åˆ«æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡")
            continue
        
        # éšæœºæ‰“æ•£
        random.shuffle(images)
        
        # ç¡®ä¿æ¯ä¸ªç±»åˆ«è‡³å°‘æœ‰1å¼ éªŒè¯å›¾ç‰‡
        if len(images) < 2:
            print(f"âš ï¸  è­¦å‘Š: {cls} ç±»åˆ«å›¾ç‰‡å¤ªå°‘({len(images)}å¼ )ï¼Œå…¨éƒ¨ç”¨ä½œè®­ç»ƒ")
            split_idx = len(images)
        else:
            # 80%ç”¨äºè®­ç»ƒï¼Œ20%ç”¨äºéªŒè¯ï¼Œä½†è‡³å°‘ä¿ç•™1å¼ ç”¨äºéªŒè¯
            split_idx = max(1, int(len(images) * 0.8))
            if split_idx == len(images):
                split_idx = len(images) - 1
        
        # æ·»åŠ è®­ç»ƒæ ·æœ¬
        for img in images[:split_idx]:
            rel_path = os.path.relpath(img, dataset_root)
            train_list.append(f"{rel_path}\t{label_map[cls]}")
        
        # æ·»åŠ éªŒè¯æ ·æœ¬
        for img in images[split_idx:]:
            rel_path = os.path.relpath(img, dataset_root)
            val_list.append(f"{rel_path}\t{label_map[cls]}")
    
    # æ‰“æ•£è®­ç»ƒå’ŒéªŒè¯åˆ—è¡¨
    random.shuffle(train_list)
    random.shuffle(val_list)
    
    # ä¿å­˜åˆ—è¡¨æ–‡ä»¶
    with open("train_list.txt", "w", encoding="utf-8") as f:
        for item in train_list:
            f.write(f"{item}\n")
    
    with open("val_list.txt", "w", encoding="utf-8") as f:
        for item in val_list:
            f.write(f"{item}\n")
    
    # åˆ›å»ºç±»åˆ«åç§°æ˜ å°„æ–‡ä»¶
    with open("class_names.txt", "w", encoding="utf-8") as f:
        for cls in classes:
            f.write(f"{cls}\n")
    
    print(f"\nâœ… æ•°æ®é›†å‡†å¤‡å®Œæˆ:")
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_list)} å¼ ")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_list)} å¼ ") 
    print(f"  è®­ç»ƒ/éªŒè¯æ¯”ä¾‹: {len(train_list)/(len(train_list)+len(val_list)):.1%} / {len(val_list)/(len(train_list)+len(val_list)):.1%}")
    
    # æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§
    print(f"\nğŸ“ˆ æ•°æ®å¹³è¡¡æ€§åˆ†æ:")
    min_count = min(class_stats.values())
    max_count = max(class_stats.values())
    print(f"  æœ€å°‘: {min_count} å¼ ")
    print(f"  æœ€å¤š: {max_count} å¼ ") 
    print(f"  å¹³è¡¡æ¯”: {min_count/max_count:.2f}")
    
    if min_count / max_count < 0.1:
        print("  âš ï¸  æ•°æ®ä¸å¹³è¡¡ä¸¥é‡ï¼Œå»ºè®®è¿›è¡Œæ•°æ®å¢å¼º")
    
    return classes, len(train_list), len(val_list)

def check_environment():
    """
    æ£€æŸ¥è®­ç»ƒç¯å¢ƒ
    """
    print("ğŸ”§ æ£€æŸ¥è®­ç»ƒç¯å¢ƒ...")
    
    # æ£€æŸ¥PaddleXç‰ˆæœ¬
    try:
        import paddlex as pdx
        print(f"âœ… PaddleXç‰ˆæœ¬: {pdx.__version__}")
    except Exception as e:
        print(f"âŒ PaddleXå¯¼å…¥å¤±è´¥: {e}")
        return False
    
    # æ£€æŸ¥æ•°æ®é›†è·¯å¾„
    dataset_path = "../../../dataset"
    if not os.path.exists(dataset_path):
        print(f"âŒ æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨: {dataset_path}")
        return False
    else:
        print(f"âœ… æ•°æ®é›†è·¯å¾„: {dataset_path}")
    
    # æ£€æŸ¥å¯ç”¨å†…å­˜å’ŒGPU
    try:
        import psutil
        memory = psutil.virtual_memory()
        print(f"âœ… å¯ç”¨å†…å­˜: {memory.available // (1024**3):.1f} GB / {memory.total // (1024**3):.1f} GB")
    except:
        print("âš ï¸  æ— æ³•æ£€æŸ¥å†…å­˜ä¿¡æ¯")
    
    try:
        import paddle
        if paddle.is_compiled_with_cuda():
            print("âœ… GPUæ”¯æŒ: å·²å¯ç”¨")
        else:
            print("âš ï¸  GPUæ”¯æŒ: æœªå¯ç”¨ï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
    except:
        print("âš ï¸  æ— æ³•æ£€æŸ¥GPUæ”¯æŒ")
    
    return True

def create_quick_test():
    """
    åˆ›å»ºå¿«é€Ÿæµ‹è¯•è„šæœ¬
    """
    test_code = '''
# å¿«é€Ÿæµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
import paddlex as pdx
import os

def test_model():
    model_path = "./inference_model_new"  # æˆ– "./inference_model_simple" 
    
    if not os.path.exists(model_path):
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        # åŠ è½½æ¨¡å‹
        predictor = pdx.create_predictor(model_path)
        
        # æµ‹è¯•å›¾ç‰‡è·¯å¾„
        test_image = "../../../dataset/Abudefduf_vaigiensis/fish_000001719594_03397.png"
        
        if os.path.exists(test_image):
            result = predictor.predict([test_image])
            print("é¢„æµ‹ç»“æœ:", result)
        else:
            print("æµ‹è¯•å›¾ç‰‡ä¸å­˜åœ¨")
            
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    test_model()
'''
    
    with open("test_model.py", "w", encoding="utf-8") as f:
        f.write(test_code)
    
    print("âœ… å·²åˆ›å»ºæµ‹è¯•è„šæœ¬: test_model.py")

if __name__ == "__main__":
    print("ğŸŸ æµ·æ´‹é±¼ç±»è¯†åˆ«æ¨¡å‹è®­ç»ƒç³»ç»Ÿ")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·è§£å†³ä¸Šè¿°é—®é¢˜åé‡è¯•")
        exit(1)
    
    print("\n" + "=" * 50)
    print("ğŸ”„ å¼€å§‹å‡†å¤‡æ•°æ®é›†...")
    
    # 2. å‡†å¤‡æ•°æ®é›†æ–‡ä»¶  
    try:
        classes, train_count, val_count = prepare_dataset_files()
        
        if train_count == 0:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®")
            exit(1)
            
    except Exception as e:
        print(f"âŒ æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
        exit(1)
    
    print("\n" + "=" * 50)
    print("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    
    # 3. è®­ç»ƒæ¨¡å‹
    try:
        success = train_fish_classification_model()
        
        if success:
            print("\n" + "=" * 50)
            print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
            
            # åˆ›å»ºæµ‹è¯•è„šæœ¬
            create_quick_test()
            
            print("\nğŸ“‹ ä¸‹ä¸€æ­¥:")
            print("1. è¿è¡Œ 'python test_model.py' æµ‹è¯•æ¨¡å‹")
            print("2. å°†è®­ç»ƒå¥½çš„æ¨¡å‹æ›¿æ¢åˆ° './core/inference_model_new'")
            print("3. æ›´æ–°åç«¯ä»£ç ä½¿ç”¨æ–°æ¨¡å‹")
            
        else:
            print("âŒ è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        print("å»ºè®®:")
        print("- æ£€æŸ¥æ•°æ®é›†æ ¼å¼")
        print("- é™ä½batch_size")
        print("- ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†…å­˜å’Œå­˜å‚¨ç©ºé—´")
